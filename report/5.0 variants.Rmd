<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return ''+n}
}
}});</script>

# Gradient Descent Variants
In this section, we will implement, tune, anid evaluate the three primary variants of gradient descent: 
* batch gradient descent,    
* stochastic gradient descent, and    
* mini-batch gradient descent.

We will survey their strengths and implementation challenges.

Once we've covered the basics, we will also explore some algorithms that have been widely used by the Deep Learning community to address some of the aforementioned challenges:      
* Momentum - [@sutton:problems]  
* Nesterov Accelerated Gradient [@nesterov1983]    
* Adagrad [@DuchiJDUCHI2011]     
* Adadelta [@Zeiler]      
* RMSProp [@Hinton2012]      
* Adam [@KingmaB14]     
* AdaMax [@KingmaB14]         
* Nadam [@Dozat]  
