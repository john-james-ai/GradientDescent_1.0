<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return ''+n}
}
}});</script>

```{python GradientDescent, code=readLines('../src/GradientDescent.py'), message=FALSE, warning=FALSE}
```
```{python GradientDemo, code=readLines('../src/GradientDemo.py'), message=FALSE, warning=FALSE}
```
```{python GradientFit, code=readLines('../src/GradientFit.py'), message=FALSE, warning=FALSE}
```
```{python GradientLab, code=readLines('../src/GradientLab.py'), message=FALSE, warning=FALSE}
```

```{python bgd_defaults, echo=F, eval=T}
directory = "./report/figures/BGD/Lab"
```

## Batch Gradient Descent 
Batch gradient descent (BGD) updates the parameters $\theta$ after computing the gradient on the entire training set. This is the key difference between batch gradient descent and its variants. 

The advantage of computing the gradient on the entire dataset is that it produces a better, more stable approximation of the gradient. 

On the other hand, batch gradient descent has its challenges:    

* The stability of the gradient may result in a suboptimal state of convergence when local minimums or saddle points are encountered. A more noisy gradient can enable the algorithm to escape saddle points or subvert local minima.     
* Computing the gradient on the entire dataset often results in a slower convergence, especially for large datasets.    
* Batch gradient descent requires the entire dataset to be memory resident, which can be problematic for large datasets.   
* Classic batch gradient descent works on a constant learning rate for all iterations of the search. Ideally, we would like a learning rate to adapt to the contours of our objective function. Many textbook optimization algorithms use analytic approaches, such as Newton's method, to determine the exact step size. However, the computation can be too expensive to be used in the context of large neural networks. Most practitioners simply fix it to a constant or slowly decay it over iterations. Determining optimal learning rates is an area of open research.

In the following sections, we will review the algorithm, implement a basic batch gradient descent class, perform hyperparameter tuning using a gridsearch, then evaluate algorithm behavior for select hyperparameter sets. 

### Algorithm
Batch gradient descent, starts with an initial $\theta$ and performs the following recursive update rule:
$$\theta_j^{k+1} := \theta_j^k - \alpha \nabla_\theta J,$$
for $k$ iterations until approximate convergence condition or a preset maximum number of iterations is reached.
The pseudocode for the algorithm looks like this. 
```{r eval=F, echo=T, tidy=F}
Standardize our data
Initialize hyperparameters  
While stopping condition is not met 
  h = predict(data, thetas)
  c = costs(h, y, data)
  g = gradient(costs, data)
  thetas = update(thetas, learning_rate, gradient)
return (thetas)
```

### Implementation
Now, we will build the batch gradient descent class that will be used to solve our challenge problem. Before we start getting our code on, let's establish a few design considerations.

#### Design Considerations   
Our GradientDescent class will need the following methods:    

* hypothesis - computes the hypothesis given $X$ and the parameters, $\theta$.     
* cost - computes the cost given $y$ and our hypotheses.     
* gradient - calculates the gradient based upon error and the values $X_j$ for each $j$.      
* update - computes the updates to the $\theta$'s in the direction of greatest descent.    
* fit - conducts the search until stopping conditions are met     

We will support the following learning rate schedules:   

* constant learning rate, value = 'c',        
* time decay learning rates, value = 't',       
* step decay learning rates, value = 's',          
* exponential decay learning rates, value = 'e'.          

Therefore, we will need a method which updates the learning rate on each epoch accordingly. 

With respect to stopping criteria, we would want to stop the algorithm when one or more of the following conditions are met:    

* The number of iterations has reached a preset maximum,     
* Convergence hasn't improved in $i_{stop}$ consecutive iterations.

Convergence is checked once per epoch. If no validation set is provided, training set costs will be the metric by which we determine whether improvement is extant.  If a validation set is provided, our metric will be validation set mean squared error. The improvement in convergence is evaluated with a precision parameter $\epsilon$.

Lastly, since we will be building SGD and Minibatch Gradient Descent classes, we may want to create an abstract base class that contains the core functions used in the computation.

Ok, here we go.

#### Class Initialization
Let's initialize our abstract base class and concrete class for batch gradient descent. We may as well import some packages that we will need.
```{python class_init, echo=T, eval=F}
from abc import ABC, abstractmethod
import datetime
import math
import numpy as np
import pandas as pd


class GradientDescent(ABC):
    def __init__(self):
      pass
      
class BGD(GradientDescent):
    def __init__(self):
      pass    
```

Now, lets define the base classes that will be inherited by the specific batch, stochastic, and mini-batch implementations.

#### Hypothesis Method
Recall that our linear hypothesis is:
$$h_\theta(x)=\theta_0x_0+\theta_1x_1=X^T \theta.$$
Thus our hypothesis method would simply be the dot product of our inputs $X^T$ and $\theta$. We can easily compute the dot product as follows.
```{python hypothesis_bgd, echo=T, eval=F}
def _hypothesis(self, X, theta):
        return(X.dot(theta))
```

#### Error Method
Since we'll need the error for both the cost and gradient computation, let's create a method that implements:
$$e=h_\theta(x)-y$$
Taking the hypothesis from the previous method and y, we have:
```{python error_bgd, echo=T, eval=F}
def _error(self, h, y):
        return(h-y)
```

#### Cost Method
Recall our cost function is:
$$J(\theta)=\frac{1}{2m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$$
Our method simply takes half of the mean squared error as follows:
```{python cost_bgd, echo=T, eval=F}
def _cost(self, e):
        return(1/2 * np.mean(e**2))
```

#### MSE Method
This method computes validation set error. For regression our metric will be mean squared error, computed as:    
$$MSE=\frac{1}{m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$$
Therefore, our validation set MSE method is as follows:
```{python mse, echo=T, eval=F}
def _mse(self, X, y, theta):
    h = self._hypothesis(X, theta)
    e = self._error(h,y)
    return(np.mean(e**2))
```

#### Gradient Method
Next, we compute the gradient from:
$$
\nabla_\theta J =\frac{\partial}{\partial \theta_j}J(\theta)  = \frac{1}{m}\displaystyle\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})\cdot x^{(i)}_j \quad\quad \text{for every j}
$$
Note, $x_0^{(i)}$ for each $i$ is 1 - convenience to simplify our computation. 

This is a vector valued function. This means that it returns a vector of size $n$, where $n$ is the number of parameters in $X$. This the direction of steepest ascent in the objective function.
```{python gradient_bgd, echo=T, eval=F}
def _gradient(self, X, e):
        return(X.T.dot(e)/X.shape[0])
```

#### Parameter Update Method
Our update function, you recall, looks like this.
$$\theta_j^{k+1} := \theta_j^k - \alpha \nabla_\theta J$$
Our update method is therefore:
```{python update_bgd, echo=T, eval=F}
def _update(self, theta, learning_rate, gradient):
        return(theta-(learning_rate * gradient))
```


#### Update Learning Rate Method
This method updates the learning rate in accordance with the learning rate hyperparameters.
```{python update_learning_rate, echo=T, eval=F}
def _update_learning_rate(self, learning_rate_init, learning_rate_schedule, learning_rate, 
                          time_decay, step_decay, step_epochs, exp_decay, epoch):

    if learning_rate_sched == 'c':  # Constant learning rate
        learning_rate_new = learning_rate
    elif learning_rate_sched == 't':  # Time decay learning rate
        learning_rate_new = learning_rate_init/(1+time_decay*epoch)            
    elif learning_rate_sched == 's':  # Step decay learning rate
        learning_rate_new = learning_rate_init*math.pow(step_decay, math.floor((1+epoch)/step_epochs))
    elif learning_rate_sched == 'e':  # Exponential decay learning rate
        learning_rate_new = learning_rate_init * math.exp(-exp_decay*epoch)
    return(learning_rate_new)
```

#### Finished Method
This method determines when the algorithm should stop. The algorithm stops of a maximum number of iterations has been reached, or there has been no improvement in the objective function over $i_{stop}$ iterations in a row.

```{python, finished, echo=T, eval=F}
def _finished(self, state, maxiter, precision, improvement):
    if state['iteration'] == maxiter:
        return(True)        
    if abs(state['prior']-state['current']) < precision:
        self._iter_no_change += 1
        if self._iter_no_change >= improvement:
            return(True)
        else:
            return(False)
    else:
        self._iter_no_change = 0   
        return(False)

```


we will also have a method that updates the state dictionary on each iteration. 
```{python, update_state, echo=T, eval=F}
def _update_state(self,state, cross_validated, iteration, J, mse):
    state['iteration'] = iteration
    state['prior'] = state['current']
    if cross_validated:
        state['current'] = mse
    else:
        state['current'] = J
    return(state)
```

Now, we can define the fit method for our BGD class.

#### Fit Method
Finally, the workhorse of our algorithm. 
```{python fit_bgd, echo=T, eval=F}
class BGD(GradientDescent):
  def __init__(self):
      pass

  def fit(self, X, y, theta, X_val=None, y_val=None, learning_rate=0.01, 
          learning_rate_sched = 'c', time_decay=None, step_decay=None,
          step_epochs=None, exp_decay=None, maxiter=0, precision=0.001, 
          no_improvement_stop=5, scaler='minmax'):
             
      # Initialize search variables
      iteration = 0
      self._iter_no_change = 0
      
      # Initialize State 
      state = {'prior':1, 'current':10, 'iteration':0}
      
      # Set cross-validated flag if validation set included 
      cross_validated = all(v is not None for v in [X_val, y_val])        
      
      while not self._finished(state):
          iteration += 1
          mse = None

          # Compute the costs and validation set error (if required)
          h = self._hypothesis(X, theta)
          e = self._error(h, y)
          J = self._cost(e)
          if cross_validated:
              mse = self._mse(X_val, y_val, theta)            

          # Compute gradient
          g = self._gradient(X, e)

          # Update thetas 
          theta = self._update(theta, learning_rate, g)

          # Update learning rate
          learning_rate = self._update_learning_rate(learning_rate_init=learning_rate_init, 
                          learning_rate_schedule=learning_rate_schedule, 
                          learning_rate=learning_rate, time_decay=time_decay, 
                          step_decay=step_decay, step_epochs=step_epochs, 
                          exp_decay=exp_decay, epoch=iteration)

          # Update state vis-a-vis training set costs and validation set mse 
          state = self._update_state(state, iteration, J, mse)

      d = dict()
      d['theta'] = theta
      d['J'] = J
      d['mse'] = mse
  
      return(d)
```

Once we've initialized some variables, we iteratively:    

1. compute training set cost, gradient, and validation set MSE if cross_validated      
2. update the thetas     
3. update the learning rate according to the learning rate schedule  
4. update the state variables

Once convergence criteria are met, we return our thetas and costs.

That's it, for the most part. The complete class definitions are available at https://github.com/DecisionScients/GradientDescent.

Next, we will learn how to set the hyperparameters that will determine algorithm performance.

### Tuning Batch Gradient Descent
Our hyperparameters for batch gradient descent are:  
* **Initial condition hyperparameters**:    
    - initial weight parameters $\theta$

* **learning rate hyperparameters**:    
    - learning rate $\alpha$, this will be the initial learning rate for learning rate schedules    
    - learning rate schedule, which can be:      
        * 'c' for constant learning rate,    
        * 't' for time decay learning rates,      
        * 's' for step decay learning rates, and      
        * 'e' for exponential decay learning rates
    - other learning rate hyperparameters such as:     
        * time_decay: the parameter that determines the amount the learning rate decays on each iteration,     
        * step_decay: the parameter that determines the amount the learning rate decays on each step,     
        * step_epochs: the parameter that specifies the number of epochs in each step,     
        * exp_decay: the amount of exponential decay to apply to the learning rate on each iteration.     

* **stopping condition hyperparameters**:
    - precision parameter $\epsilon$ by which we evaluate whether a quantum, e.g. training set costs, has changed,   
    - $i_{stop}$, the algorithm stops when the number of consecutive iterations of no improvement equals $i_{stop}$.

Of the hyperparameters listed above, the learning rate related hyperparameters have the greatest effect on performance. We will therefore use a gridsearch strategy to create candidate functions $f_\theta(x)$ for a range of learning rate schedules and learning rates. The initial $\theta$'s, precision parameter, $\epsilon$ and the $i_{stop}$, a.k.a. the no improvement stop parameter will be set to their default values.  Stopping condition hyperparameters will be adjusted as needed. 

#### Data Preparation
As stated above, our training set is comprised of 500 observations, randomly selected from the Ames Housing Project Training Set. The remaining 960 observations in the Ames Housing Project Training Set will be allocated to our validation set. 

So, let's grab our data. We have a small function that was created to randomly sample the training data and split it into the appropriate training and validation sets.
```{python bgd_gs_data, echo=T, eval=T}
X, X_val, y, y_val = data.demo(n=500)
```

Gradient descent is sensitive to feature scaling so let's scale our data to the range [0,1] using sklearn's preprocessing module.
```{python bgd_scale, echo=T, eval=F}
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X = scaler.fit_transform(X)
X_val = scaler.fit_transform(X_val)
y = scaler.fit_transform(y)
y_val = scaler.fit_transform(y_val)
```

With that, we are ready to start the tuning process. Let's begin with a learning rate test.

#### Learning Rate Test
What is a learning rate test and why would we conduct one? As we seek to find an optimal constant learning rate, we need an upper bound on the learning rates to evaluate. This upper bound is the maximum learning rate that still improves performance of the objective function. This maximum also serves as our initial learning rate when performing time, step, or exponential learning rate annealing. 

So, let's fix our initial $\theta$'s, precision parameter $\epsilon$, and $i_{stop}$, a.k.a. no_improvement_stop parameter to their default values. We will evaluate a range of learning rates from 0 to 2 incrementing by 0.01.

```{python bgd_learning_rate_test_params, echo=T, eval=T}
theta = np.array([-1,-1]) 
learning_rate_sched = ['c']
learning_rate = np.arange(0,2, 0.01)
precision = [0.001]
maxiter = 5000
no_improvement_stop=[5]
```
We will stop the algorithm when we observe 5 consecutive iterations with no improvement in the objective function computed on the training set. 

With that we can run our gridsearch on our learning rates to determine the maximum learning rate that improves performance of the objective function. A class,  BGDLab, was created to perform our gridsearch. Let's instantiate the class, run the gridsearch and obtain some diagnostic information. 
```{python bgd_learning_rate_test, echo=T, eval=T, cache=T}
lab = BGDLab()
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, learning_rate=learning_rate, 
               learning_rate_sched=learning_rate_sched, time_decay=None, 
               step_decay=None, step_epochs=None, exp_decay=None,
               precision=precision, maxiter=maxiter, no_improvement_stop=no_improvement_stop)
```

Now, let's evaluate our training set costs vis-a-vis learning rate. 
```{python bgd_learning_rate_test_plot, echo=F, eval=T, cache=T}
filename = 'Batch Gradient Descent - Learning Rate Test - Costs.png'
lab.figure(data=lab.summary(), x='learning_rate', y='final_costs', 
           func=lab.lineplot, directory=directory, 
           filename=filename, width=0.5)  
```
![](../report/figures/BGD/Lab/Batch Gradient Descent - Learning Rate Test - Costs.png)

`r kfigr::figr(label = "bgd_constant_alpha_plots_i", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Learning Rate Test 

Training set costs in `r kfigr::figr(label = "bgd_constant_alpha_plots_i", prefix = TRUE, link = TRUE, type="Figure")` indicate divergence at a learning rate $\alpha \approx 1.6$. We'll use this as our upper bound for constant learning rates and our initial learning rate for our annealing learning rates.  

Next, let's tune our constant learning rate.

#### Constant Learning Rates
Our goal here is to identify the learning rate constant that minimizes our objective function. We'll keep initial $\theta$'s, precision parameter $\epsilon$, and $i_{stop}$, a.k.a. no_improvement_stop parameter set at their defaults. For the candidate learning rates, we will evaluate a range from 0 to 1.6, incrementing by 0.01. 
```{python bgd_constant_learning_rate_i, echo=T, eval=T}
learning_rate = np.arange(0,1.6, 0.01)
```
Again, we will stop the algorithm when we observe 5 consecutive iterations with no improvement in the objective function computed on the training set. 

##### Constant Learning Rate Analysis

Ok, let's run the search and evaluate performance. 

```{python bgd_constant_learning_rate_gs_i, echo=T, eval=T}
lab = BGDLab()
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, learning_rate=learning_rate, 
               learning_rate_sched=learning_rate_sched, time_decay=None, 
               step_decay=None, step_epochs=None, exp_decay=None,
               precision=precision, maxiter=maxiter, no_improvement_stop=no_improvement_stop)
```

```{python bgd_constant_learning_rate_plots_i, echo=F, eval=T}
filename = 'Batch Gradient Descent - GS I - Costs by Learning Rate.png'
lab.figure(data=lab.summary(), x='learning_rate', y='final_costs', 
           func=lab.lineplot, directory=directory, 
           filename=filename, width=0.5)             
```

![](../report/figures/BGD/Lab/Batch Gradient Descent - GS I - Costs by Learning Rate.png)

`r kfigr::figr(label = "bgd_constant_learning_rate_plots_i", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Training Costs by Constant Learning Rate I

`r kfigr::figr(label = "bgd_constant_learning_rate_plots_i", prefix = TRUE, link = TRUE, type="Figure")` shows high costs at each end of the spectrum of learning rates. To ascertain the minimum cost learning rate, we'll need to see what happens in learning rates $0.4 < \alpha < 1.4$. 

##### Constant Learning Rate Fine Tune
Let's adjust our learning rates and rerun the search.

```{python bgd_constant_learning_rate_ii, echo=T, eval=T}
learning_rate = np.arange(0.4,1.4, 0.01)
```

```{python bgd_constant_learning_rate_gs_ii, echo=T, eval=T}
lab = BGDLab()
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, learning_rate=learning_rate, 
               learning_rate_sched=learning_rate_sched, time_decay=None, 
               step_decay=None, step_epochs=None, exp_decay=None,
               precision=precision, maxiter=maxiter, no_improvement_stop=no_improvement_stop)
```

```{python bgd_constant_learning_rate_plots_ii, echo=F, eval=T}
filename = 'Batch Gradient Descent - GS II - Costs by Learning Rate.png'
lab.figure(data=lab.summary(), x='learning_rate', y='final_costs', 
           func=lab.lineplot, directory=directory, 
           filename=filename, width=0.5)  
filename = 'Batch Gradient Descent - GS II - Computation Time by Learning Rate.png'
lab.figure(data=lab.summary(), x='learning_rate', y='duration', 
           func=lab.lineplot, directory=directory, 
           filename=filename, width=0.5)
filename = 'Batch Gradient Descent - GS II - Report.png'
lab.report(sort='t', directory=directory, filename=filename)           
```

![](../report/figures/BGD/Lab/Batch Gradient Descent - GS II - Costs by Learning Rate.png)![](../report/figures/BGD/Lab/Batch Gradient Descent - GS II - Computation Time by Learning Rate.png)

`r kfigr::figr(label = "bgd_constant_learning_rate_plots_ii", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Training Costs and Computation Time by Constant Learning Rate II

```{r bgd_constant_learning_rate_report_ii, eval=T, echo=F}
constant_alpha_report <- read.csv(file.path(py$directory, py$filename))
```

Ok, we have a better sense of objective function performance vis-a-vis our learning rates. The lowest cost was observed with a learning rate of `r constant_alpha_report['learning_rate'][1,]`. This coincides with a computation time of `r round(constant_alpha_report['duration'][1,],4)` milliseconds.

##### Contant Learning Rate Evaluation
Now, let's evaluate generalizability of the learning rates. 
```{python bgd_constant_learning_rate_plots_ii_val, echo=F, eval=T}
filename = 'Batch Gradient Descent - GS II - Validation Set Error by Learning Rate.png'
lab.figure(data=lab.summary(), x='learning_rate', y='final_mse', 
           func=lab.lineplot, directory=directory, 
           filename=filename, width=0.5)        
```
![](../report/figures/BGD/Lab/Batch Gradient Descent - GS II - Validation Set Error by Learning Rate.png)

`r kfigr::figr(label = "bgd_constant_learning_rate_plots_ii_val", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Validation Set Error by Constant Learning Rate II

```{r bgd_constant_learning_rate_report_ii_val, eval=T, echo=F}
constant_alpha_report <- constant_alpha_report %>% arrange(final_mse, duration)
```
Indeed, our minimum learning rate $\alpha=1.24$ also produced the lowest validation set error. The hyperparameter set and its results are:    

```{r bgd_constant_learning_rate_final_params, echo=F, eval=T}
knitr::kable(list(t(constant_alpha_report[1,])), format.args = list(big.mark = ",")) %>%
 kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = F, position = "center")
```

##### Constant Learning Rate Convergence Visualization
Let's inspect the training set path to convergence for our selected hyperparameters.  
```{python bgd_constant_learning_rate_convergence, echo=F, eval=T, cache=T}
theta = np.array([-1,-1]) 
learning_rate_sched = 'c'
learning_rate = 1.24
precision = 0.001
maxiter = 5000
no_improvement_stop=5

constant_lr_demo = BGDDemo()
constant_lr_demo.fit(X=X,y=y, theta=theta, learning_rate=learning_rate, 
         learning_rate_sched=learning_rate_sched,
         precision=precision, maxiter=maxiter, 
         no_improvement_stop=no_improvement_stop)

constant_lr_demo.show_search(directory=directory, fps=1)
constant_lr_demo.show_fit(directory=directory, fps=1)
```
![](../report/figures/BGD/Lab/Batch Gradient Descent Search Plot Learning Rate 1.24.gif)

`r kfigr::figr(label = "bgd_constant_learning_rate_convergence", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Constant Learning Rate = 1.24 Convergence

The convergence path is characteristic of large learning rates. Rather chaotic, the convergence path oscillates sharply around the minimum. The approximate convergence criteria is met within 7 epochs, yielding a training set cost of $J\approx0.03$. Let's see how the solution fits the data. 

![](../report/figures/BGD/Lab/Batch Gradient Descent Fit Plot Learning Rate 1.24.gif)

`r kfigr::figr(label = "bgd_constant_learning_rate_fit", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Constant Learning Rate = 1.24 Fit

The regression line makes early and rapid advances towards a fit of the data; however, the final regression line reveals a suboptimal fit.  What's happening here?

The challenge with this optimization problem is that of the *vanishing gradient*. Note the gradients (slope of the regression lines) in `r kfigr::figr(label = "bgd_constant_learning_rate_fit", prefix = TRUE, link = TRUE, type="Figure")` oscillates and approaches zero with each iteration. Since the step size is scaled by the magnitude of the gradient, the size of each step diminishes with gradient. Hence, the corresponding change in the cost function decays, ultimately to a value less than $\epsilon$ and the search stops early. 

##### Constant Learning Rate Fine Tune 2.0
How do we remedy this? We need to buy more time for the algorithm to find a more optimal solution. This can be accomplished by lowering the precision parameter $\epsilon$, or increasing the $i_{stop}$ parameter, or both. Let's run our gridsearch again, but this time we will expand our parameter space to include several values for the $i_{stop}$ parameter.
```{python bgd_constant_learning_rate_params_iii, echo=T, eval=T}
theta = np.array([-1,-1]) 
learning_rate_sched = ['c']
learning_rate = np.arange(0.5,1.4, 0.01)
precision = [0.001]
maxiter = 5000
no_improvement_stop=[5,10,20]
```

Ok, let's run our gridsearch.
```{python bgd_constant_learning_rate_gs_iii, echo=T, eval=T}
lab = BGDLab()
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, learning_rate=learning_rate, 
               learning_rate_sched=learning_rate_sched, time_decay=None, 
               step_decay=None, step_epochs=None, exp_decay=None,
               precision=precision, maxiter=maxiter, no_improvement_stop=no_improvement_stop)
```

Let's include our $i_{stop}$ parameter in our diagnostic plot to see what impact that has on both training set costs and validation set error.
```{python bgd_constant_learning_rate_plots_iii, echo=F, eval=T}
filename = 'Batch Gradient Descent - GS III - Costs by Learning Rate.png'
lab.figure(data=lab.summary(), x='learning_rate', y='final_costs', z='no_improvement_stop',
           func=lab.lineplot, directory=directory, 
           filename=filename, width=0.5)  
filename = 'Batch Gradient Descent - GS III - Validation Error by Learning Rate.png'
lab.figure(data=lab.summary(), x='learning_rate', y='final_mse', z='no_improvement_stop',
           func=lab.lineplot, directory=directory, 
           filename=filename, width=0.5)             
filename = 'Batch Gradient Descent - GS III - Report.png'
lab.report(sort='t', directory=directory, filename=filename)           
```

![](../report/figures/BGD/Lab/Batch Gradient Descent - GS III - Costs by Learning Rate.png)![](../report/figures/BGD/Lab/Batch Gradient Descent - GS III - Validation Error by Learning Rate.png)

`r kfigr::figr(label = "bgd_constant_learning_rate_plots_iii", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Costs by Constant Learning Rate
```{r bgd_constant_learning_rate_report_iii, eval=T, echo=F}
constant_alpha_report_iii <- read.csv(file.path(py$directory, py$filename))
constant_alpha_report_iii_costs <- constant_alpha_report_iii %>% arrange(final_costs, duration)
constant_alpha_report_iii_error <- constant_alpha_report_iii %>% arrange(final_mse, duration)
```

It would appear that our $i_{stop}$ a.k.a. the no improvement stop parameter has a dramatic effect on both training set costs and validation set error. A higher value $i_{stop}$ results in continued decrease in costs at learning rates above 1.2. In fact, we have a minimum training set costs of `r round(constant_alpha_report_iii_costs['final_costs'][1,],4)` and a minimum validation set error of `r round(constant_alpha_report_iii_error['final_mse'][1,],4)` both occurring at learning rate `r constant_alpha_report_iii_error['learning_rate'][1,]`

Let's evaluate convergence with the new parameters.

##### Constant Learning Rate Convergence 2.0
Raising the $i_{stop}$ parameter increased the number of iterations to convergence from 7 to 23. Final training set costs was $J\approx0.02$. 
```{python bgd_constant_learning_rate_convergence_ii, echo=F, eval=T, cache=T}
theta = np.array([-1,-1]) 
learning_rate_sched = 'c'
learning_rate = 1.39
precision = 0.001
maxiter = 5000
no_improvement_stop=20

constant_lr_demo = BGDDemo()
constant_lr_demo.fit(X=X,y=y, theta=theta, learning_rate=learning_rate, 
         learning_rate_sched=learning_rate_sched,
         precision=precision, maxiter=maxiter, 
         no_improvement_stop=no_improvement_stop)

constant_lr_demo.show_search(directory=directory, fps=1)
constant_lr_demo.show_fit(directory=directory, fps=1)
```
![](../report/figures/BGD/Lab/Batch Gradient Descent Search Plot Learning Rate 1.39.gif)

`r kfigr::figr(label = "bgd_constant_learning_rate_convergence_ii", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Constant Learning Rate = 1.39 Convergence

This convergence path differs from that in `r kfigr::figr(label = "bgd_constant_learning_rate_convergence", prefix = TRUE, link = TRUE, type="Figure")` in one notable respect. Significantly more cycles were spent optimizing in the direction slope parameter $\theta_1$. Let's see how the new parameters fit the data.

##### Constant Learning Rate Fit 2.0
The fit to the data has markedly improved. Creating a more restricted stopping condition allowed us to raise the learning rate while decreasing both training set cost and validation set error. 

![](../report/figures/BGD/Lab/Batch Gradient Descent Fit Plot Learning Rate 1.39.gif)

`r kfigr::figr(label = "bgd_constant_learning_rate_fit_ii", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Constant Learning = 1.39 Rate Fit

The additional optimization time in the $\theta_1$ direction enabled the algorithm to escape the vanishing gradient and arrive at a more optimal solution.

