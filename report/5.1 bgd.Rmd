<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return ''+n}
}
}});</script>

## Batch Gradient Descent 
Batch gradient descent, starts with an initial $\theta$ and performs the following recursive update rule:
$$\theta_j^{k+1} := \theta_j^k - \alpha \nabla_\theta J,$$
for $k$ iterations until approximate convergence condition or a preset maximum number of iterations is reached.
The pseudocode for the algorithm looks like this. 
```{r eval=F, echo=T, tidy=F}
Standardize our data
Initialize hyperparameters  
While stopping condition is not met 
  h = predict(data, thetas)
  c = costs(h, y, data)
  g = gradient(costs, data)
  thetas = update(thetas, learning_rate, gradient)
return (thetas)
```
`r kfigr::figr(label = "gd_algorithm", prefix = TRUE, link = TRUE, type="Figure")`: Gradient Descent Algorithm

### Advantages of Batch Gradient Descent
Batch gradient descent updates the parameters $\theta$ after computing the gradient on the entire training set. This is the key difference between batch gradient descent and its variants. Using the entire data set  provides a better, more stable approximation of the gradient.

### Challenges of Batch Gradient Descent
Since batch gradient descent must compute average gradients over the entire dataset on each iteration, it is often slower to converge than its variants on large data sets. The whole of the data set must also be maintained in memory, which can be problematic for large data sets.

Moreover, the stability of the gradient may result in a suboptimal state of convergence. This will become clearer in our experiments below.

Lastly, 'classic' batch gradient descent works on a constant learning rate for all iterations of the search. Ideally, we would like a learning rate to adapt to the contours of our objective function. Many textbook optimization algorithms use analytic approaches, such as Newton's method, to determine the exact step size. However, the computation can be too expensive to be used in the context of large neural networks. Most practitioners simply fix it to a constant or slowly decay it over iterations. Determining optimal learning rates is an area of open research.

### Implementation
Now, we will build the batch gradient descent class that will be used to solve our challenge problem. Before we start getting our code on, let's establish a few design considerations:

#### Design Considerations   
Our GradientDescent class will need the following methods:    
* hypothesis - computes the hypothesis given $X$ and the parameters, $\theta$.     
* cost - computes the cost given $y$ and our hypotheses.     
* gradient - calculates the gradient based upon error and the values $X_j$ for each $j$.      
* update - computes the updates to the $\theta$'s in the direction of greatest descent.    
* fit - conducts the search until stopping conditions are met     

With respect to stopping criteria, we would want to stop the algorithm when one or more of the following conditions are met:    
* The number of iterations has reached a preset maximum,     
* The relative or absolute change in the training set costs have fallen below a preset precision,     
* The relative or absolute change in the validation set costs have fallen below a preset precision,       
* The percent or absolute change in the gradient has fallen below a preset precision, and           
* The validation set costs has reached a preset maximum (in the case the algorithm starts to diverge).     

Lastly, since we will be building SGD and Minibatch Gradient Descent classes, we may want to create an abstract base class that contains the core functions used in the computation.

Ok, here we go.

#### Class Initialization
Let's initialize our abstract base class and concrete class for batch gradient descent. We may as well import some packages that we will need.
```{python class_init, echo=T, eval=F}
from abc import ABC, abstractmethod
import datetime
import math
import numpy as np
import pandas as pd


class GradientDescent(ABC):
    def __init__(self):
      pass
      
class BGD(GradientDesent):
    def __init__(self):
      pass    
```
`r kfigr::figr(label = "class_init", prefix = TRUE, link = TRUE, type="Figure")` Class Initialization

Now, lets define the base classes that will be inherited by the specific batch, stochastic, and mini-batch implementations.

#### Hypothesis Method
Recall that our linear hypothesis is:
$$h_\theta(x)=\theta_0x_0+\theta_1x_1=X^T \theta.$$
Thus our hypothesis method would simply be the dot product of our inputs $X$ and $\theta$. We can easily compute the dot product as follows.
```{python hypothesis_bgd, echo=T, eval=F}
def _hypothesis(X, theta):
        return(X.dot(theta))
```
`r kfigr::figr(label = "hypothesis_bgd", prefix = TRUE, link = TRUE, type="Figure")` Hypothesis Method

#### Error Method
Since we'll need the error for both the cost and gradient computation, let's create a method that implements:
$$e=h_\theta(x)-y$$
Taking the hypothesis from the previous method and y, we have:
```{python error_bgd, echo=T, eval=F}
def _error(h, y):
        return(h-y)
```
`r kfigr::figr(label = "error_bgd", prefix = TRUE, link = TRUE, type="Figure")` Error Method

#### Cost Method
Recall our cost function is:
$$J(\theta)=\frac{1}{2m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$$
Our method simply takes half of the mean squared error as follows:
```{python cost_bgd, echo=T, eval=F}
def _cost(self, e):
        return(1/2 * np.mean(e**2))
```
`r kfigr::figr(label = "cost_bgd", prefix = TRUE, link = TRUE, type="Figure")` Cost Method

#### Gradient Method
Next, we compute the gradient from:
$$
\nabla_\theta J =\frac{\partial}{\partial \theta_j}J(\theta)  = \frac{1}{m}\displaystyle\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})\cdot x^{(i)}_j \quad\quad \text{for every j}
$$
Note, $x_0^{(i)}$ for each $i$ is 1 - convenience to simplify our computation. 

This is a vector valued function. This means that it returns a vector of size $n$, where $n$ is the number of parameters in $X$. This the direction of steepest ascent in the objective function.
```{python gradient_bgd, echo=T, eval=F}
def _gradient(self, X, e):
        return(X.T.dot(e)/X.shape[0])
```
`r kfigr::figr(label = "gradient_bgd", prefix = TRUE, link = TRUE, type="Figure")` Gradient Method

#### Update Method
Our update function, you recall, looks like this.
$$\theta_j^{k+1} := \theta_j^k - \alpha \nabla_\theta J$$
Our update method is therefore:
```{python update_bgd, echo=T, eval=F}
def _update(self, alpha, theta, gradient):
        return(theta-(alpha * gradient))
```
`r kfigr::figr(label = "update_bgd", prefix = TRUE, link = TRUE, type="Figure")` Update Method

#### Finished Method
This method determines when the algorithm should stop. Again, we stop if the maximum number of iterations has been reached, or the absolute or relative change in our stopping measure has fallen below our precision parameter. We'll store the current state of the search in a dictionary.  It will contain the current iteration as well as the current and prior values of our evaluation metric.

```{python, finished, echo=T, eval=F}
def _finished(self, maxiter, stop_metric, precision, state):
    if maxiter:
        if state['iteration'] == maxiter:
            return(True)
    elif stop_metric == 'a': # Absolute change
        return(abs(state['prior']-state['current']) < precision)
    else: # Relative change
        return(abs(state['prior']-state['current'])/abs(state['prior'])*100 < precision)    

```
`r kfigr::figr(label = "finished_bgd", prefix = TRUE, link = TRUE, type="Figure")` Finished Method

we will also have a method that updates the state dictionary on each iteration. 
```{python, update_state, echo=T, eval=F}
def _update_state(self,state, iteration, J, J_val, g):
    state['iteration'] = iteration
    state['prior'] = state['current']
    if stop_parameter == 't':   # t for training Set Costs
        state['current'] = J
    elif stop_parameter == 'v': # v for validation set costs
        state['current'] = J_val
    else:                       # gradient
        state['current'] = np.sqrt(np.sum(abs(g)**2)) # Computes the norm of the gradient
    return(state)  

```
`r kfigr::figr(label = "update_state", prefix = TRUE, link = TRUE, type="Figure")` Update State

Now, we can define the fit method for our BGD class.

#### Fit Method
Finally, the workhorse of our algorithm. 
```{python fit_bgd, echo=T, eval=F}
class BGD(GradientDescent):
  def __init__(self):
      pass

  def fit(self, X, y, theta, X_val=None, y_val=None, 
             alpha=0.01, maxiter=0, precision=0.001,
             stop_measure='j', stop_metric='a', max_costs=100):
             
      state = {'iteration':0, 'prior':1, 'current':5}
      J_val = None
  
      # Set cross-validated flag if validation set included 
      cross_validated = all(v is not None for v in [X_val, y_val])
  
      while not self._finished(state):
          iteration += 1
  
          # Compute the costs and validation set error (if required)
          h = self._hypothesis(X, theta)
          e = self._error(h, y)
          J = self._cost(e)
          g = self._gradient(X, e)
  
          if cross_validated:
            h_val = self._hypothesis(X_val, theta)
            e_val = self._error(h_val, y_val)
            J_val = self._cost(e_val)                
            
          theta = self._update(alpha, theta, g)
            
          state = self._update_state(state, iteration, J, J_val, g)          
  
      d = dict()
      d['theta'] = theta
      d['J'] = J
      d['J_val'] = J_val
  
      return(d)
```
`r kfigr::figr(label = "fit_bgd", prefix = TRUE, link = TRUE, type="Figure")` Fit Method
Once we've initialized some variables, we iteratively:    
1. compute training set cost and gradient,   
2. compute validation set cost if we are using a cross-validation set,    
3. update the thetas    
4. update our state dictionary    

Once convergence criteria are met, we return our thetas and costs.

That's it, for the most part. The complete class definitions are available at https://github.com/DecisionScients/GradientDescent.

Next, we will learn how to set the hyperparameters that will determine algorithm performance.

```{python GradientDescent, code=readLines('../src/GradientDescent.py'), message=FALSE, warning=FALSE}
```
```{python GradientDemo, code=readLines('../src/GradientDemo.py'), message=FALSE, warning=FALSE}
```
```{python GradientFit, code=readLines('../src/GradientFit.py'), message=FALSE, warning=FALSE}
```
```{python GradientLab, code=readLines('../src/GradientLab.py'), message=FALSE, warning=FALSE}
```
```{python BGDReport, code=readLines('../report/src/BGDReport.py'), message=FALSE, warning=FALSE}
```
### Hyperparameter Tuning via Gridsearch
The goal at this stage is to determine the set of hyperparameters that minimize cost on our training and validation sets. A gridsearch approach will allow us to quickly sweep through a range of parameter values in an exhaustive search. Once we've completed the gridsearch, we will closely examine the best and most interesting solutions.

Our hyperparameters are:    
* theta: our initial thetas,      
* alpha: our learning rate,    
* maxiter: The maximum number of iterations,    
* max_costs: The maximum cost   

* stop parameter: The parameter we will use to evaluate *near* convergence. Values are:   
  - t: training set costs,    
  - v: validation set costs,  
  - g: gradient norm     
  
* stop metric: The metric we will use when evaluating the stop parameter. Values are:     
  - a: absolute change in the stop parameter, and      
  - r: relative change in the stop parameter.     
  
* precision: the $\epsilon$ for our stopping criteria, e.g. we stop when $J_k - J_{k-1} < \epsilon$

Our training and validation sets were randomly selected from our Ames Housing Project Training Set.  The training set is comprised of random sample of 500 observations. The remaining 960 samples comprise the validation set. Our single predictor of sale price will be living area. Restricting our experiment to a single predictor will allow us to more easily visualize the algorithm's behavior. 

A small function was created to randomly sample the training data and split it into the appropriate training and validation sets.
```{python demo_data, echo=T, eval=F}
X, X_val, y, y_val = data.demo(n=500)
```

Now that we have our data, lets run our gridsearch.

#### GridSearch 
We will search a wide range of learning rates, precision parameters, and stopping conditions. Thus our parameters are:
```{python gs_params_1, echo=T, eval=F}
theta = np.array([-1,-1]) 
alpha = [0.02, 0.04, 0.06, 0.08, 0.1, 0.2, 0.4, 0.6, 0.8, 1]
precision = [0.1, 0.01, 0.001, 0.0001]
maxiter = 10000
stop_parameter = ['t', 'v', 'g']
stop_metric = ['a', 'r']
```
`r kfigr::figr(label = "gs_params_1", prefix = TRUE, link = TRUE, type="Figure")` Gridsearch Parameters

Ok, let's run our gridsearch.
```{python bgd_gs_fake, echo=T, eval=F}
lab = BGDLab()
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, alpha=alpha, precision=precision,
           maxiter=maxiter, stop_parameter=stop_parameter, stop_metric=stop_metric)
```

```{python bgd_gs, echo=F, eval=T}
report, featured = bgd_gs()
featured
```

`r kfigr::figr(label = "bgd_gs", prefix = TRUE, link = TRUE, type="Figure")` shows the validation set costs and elapsed time for `r nrow(py$report)` experiments. The best and worst in terms of costs are annotated, as are the fastest and longest in terms of computation time.

![](../report/figures/BGD/Validation Set Costs and TimeBy Learning Rate.png)
`r kfigr::figr(label = "bgd_gs", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Gridsearch

### Analysis
We've selected the best and worst in terms of validation set costs and computation time. Let's run their parameters through the model and see what we can learn about the behavior of batch gradient descent under various parameter sets. 

The experiments of interest are:
```{r featured}
#rownames(py$featured) <- NULL
# knitr::kable(py$featured, digits = 5, format.args = list(big.mark = ",")) %>%
#  kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center")
```
`r kfigr::figr(label = "featured", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Featured Solutions

#### Slowest to Converge
Let's start with solution that took the longest to converge. This solution converged when the relative change in the gradient norm fell below a $\epsilon=0.0001$. The learning rate $\alpha=0.02$. Thus the parameters are:
```{python bgd_longest_params, echo=T, eval=F}
theta = np.array([-1,-1]) 
alpha = 0.02
precision = 0.0001
maxiter = 10000
stop_parameter = 'g'
stop_metric = 'r'
```
`r kfigr::figr(label = "bgd_longest_params", prefix = TRUE, link = TRUE, type="Figure")` Longest Search Parameters

Ok, let's fire it up.
```{python bgd_longest_fake, echo=T, eval=F}
bgd = BGD()
bgd.fit(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, alpha=alpha, precision=precision,
           maxiter=maxiter, stop_parameter=stop_parameter, stop_metric=stop_metric)
```

```{python bgd_longest, echo=F, eval=T}
bgd_longest(featured.iloc[2])
```
 
![](../report/figures/BGD/slowest search.gif)
`r kfigr::figr(label = "bgd_slowest", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent - Slowest to Converge


![](../report/figures/BGD/slowest fit.gif)
`r kfigr::figr(label = "bgd_slowest_fit", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent - Slowest to Converge Fit




#### Fastest to Converge
The fastest to converge stopped when the absolute change in validation set costs dropped below $\epsilon=0.1$. The learning rate $\alpha=0.02$.
```{python bgd_fastest_params, echo=T, eval=F}
theta = np.array([-1,-1]) 
alpha = 0.2
precision = 0.1
maxiter = 10000
stop_parameter = 't'
stop_metric = 'r'
```
`r kfigr::figr(label = "bgd_fastest_params", prefix = TRUE, link = TRUE, type="Figure")` Fastest Search Parameters

Ok, let's fire it up.
```{python bgd_fastest_fake, echo=T, eval=F}
bgd = BGD()
bgd.fit(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, alpha=alpha, precision=precision,
           maxiter=maxiter, stop_parameter=stop_parameter, stop_metric=stop_metric)
```

```{python bgd_fastest, echo=F, eval=T}
bgd_fastest(featured.iloc[3])
```

![](../report/figures/BGD/fastest search.gif)
`r kfigr::figr(label = "bgd_fastest", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent - Fastest to Converge

![](../report/figures/BGD/fastest fit.gif)
`r kfigr::figr(label = "bgd_slowest_fit", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent - Fastest to Converge Fit

#### Worst Cost
The worst cost solution had a learning rate of $\alpha=0.02$ and stopped when the absolute change in training set costs, dropped below the precision $\epsilon=0.1$.
```{python bgd_worst_params, echo=T, eval=F}
theta = np.array([-1,-1]) 
alpha = 0.02
precision = 0.1
maxiter = 10000
stop_parameter = 't'
stop_metric = 'a'
```
`r kfigr::figr(label = "bgd_worst_params", prefix = TRUE, link = TRUE, type="Figure")` Worst Solution

Ok, let's fire it up.
```{python bgd_worst_fake, echo=T, eval=F}
bgd = BGD()
bgd.fit(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, alpha=alpha, precision=precision,
           maxiter=maxiter, stop_parameter=stop_parameter, stop_metric=stop_metric)
```

```{python bgd_worst, echo=F, eval=T}
bgd_worst(featured.iloc[1])
```

![](../report/figures/BGD/worst search.gif)
`r kfigr::figr(label = "bgd_worst", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent - Worst Cost

![](../report/figures/BGD/worst fit.gif)
`r kfigr::figr(label = "bgd_worst_fit", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent - Worst Cost Fit


#### Best Cost
The best cost solution had a learning rate of $\alpha=0.08$ and stopped when the absolute change in training set costs, dropped below the precision $\epsilon=0.1$.
```{python bgd_best_params, echo=T, eval=F}
theta = np.array([-1,-1]) 
alpha = 0.08
precision = 0.0001
maxiter = 10000
stop_parameter = 't'
stop_metric = 'a'
```
`r kfigr::figr(label = "bgd_best_params", prefix = TRUE, link = TRUE, type="Figure")` Best Solution

Ok, let's fire it up.
```{python bgd_best_fake, echo=T, eval=F}
bgd = BGD()
bgd.fit(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, alpha=alpha, precision=precision,
           maxiter=maxiter, stop_parameter=stop_parameter, stop_metric=stop_metric)
```

```{python bgd_best, echo=F, eval=T}
bgd_best(featured.iloc[0])
```

![](../report/figures/BGD/best search.gif)
`r kfigr::figr(label = "bgd_best", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent - Best Cost

![](../report/figures/BGD/best fit.gif)
`r kfigr::figr(label = "bgd_best_fit", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent - Best Cost Fit


