<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return ''+n}
}
}});</script>

```{python GradientDescent, code=readLines('../src/GradientDescent.py'), message=FALSE, warning=FALSE}
```
```{python GradientDemo, code=readLines('../src/GradientDemo.py'), message=FALSE, warning=FALSE}
```
```{python GradientFit, code=readLines('../src/GradientFit.py'), message=FALSE, warning=FALSE}
```
```{python GradientLab, code=readLines('../src/GradientLab.py'), message=FALSE, warning=FALSE}
```
```{python BGDReport, code=readLines('../report/src/BGDReport.py'), message=FALSE, warning=FALSE}
```

```{python bgd_demo, echo=F, eval=T}
bgd_demo(alpha=0.1, precision=0.01, maxiter=10000, miniter=150, 
         stop_parameter='v', stop_metric='r', filename = 'Demo', 
         fit=False)
```

## Batch Gradient Descent 
Batch gradient descent updates the parameters $\theta$ after computing the gradient on the entire training set. This is the key difference between batch gradient descent and its variants. 

<img src="../report/figures/BGD/Demo Search Minimum Iterations - 150.gif"/>
`r kfigr::figr(label = "bgd_search", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent 

The advantage of computing the gradient on the entire dataset is that it produces a better, more stable approximation of the gradient. 

On the other hand, batch gradient descent has its challenges:    

* The stability of the gradient may result in a suboptimal state of convergence when local minimums or saddle points are encountered. A more noisy gradient can enable the algorithm to escape saddle points or subvert local minima.     
* Computing the gradient on the entire dataset often results in a slower convergence, especially for large datasets.    
* Batch gradient descent requires the entire dataset to be memory resident, which can be problematic for large datasets.   
* Classic batch gradient descent works on a constant learning rate for all iterations of the search. Ideally, we would like a learning rate to adapt to the contours of our objective function. Many textbook optimization algorithms use analytic approaches, such as Newton's method, to determine the exact step size. However, the computation can be too expensive to be used in the context of large neural networks. Most practitioners simply fix it to a constant or slowly decay it over iterations. Determining optimal learning rates is an area of open research.

In the following sections, we will review the algorithm, implement a basic batch gradient descent class, perform hyperparameter tuning using a gridsearch, then evaluate algorithm behavior for select hyperparameter sets. 

### Algorithm
Batch gradient descent, starts with an initial $\theta$ and performs the following recursive update rule:
$$\theta_j^{k+1} := \theta_j^k - \alpha \nabla_\theta J,$$
for $k$ iterations until approximate convergence condition or a preset maximum number of iterations is reached.
The pseudocode for the algorithm looks like this. 
```{r eval=F, echo=T, tidy=F}
Standardize our data
Initialize hyperparameters  
While stopping condition is not met 
  h = predict(data, thetas)
  c = costs(h, y, data)
  g = gradient(costs, data)
  thetas = update(thetas, learning_rate, gradient)
return (thetas)
```
`r kfigr::figr(label = "gd_algorithm", prefix = TRUE, link = TRUE, type="Figure")`: Gradient Descent Algorithm

### Implementation
Now, we will build the batch gradient descent class that will be used to solve our challenge problem. Before we start getting our code on, let's establish a few design considerations.

#### Design Considerations   
Our GradientDescent class will need the following methods:    

* hypothesis - computes the hypothesis given $X$ and the parameters, $\theta$.     
* cost - computes the cost given $y$ and our hypotheses.     
* gradient - calculates the gradient based upon error and the values $X_j$ for each $j$.      
* update - computes the updates to the $\theta$'s in the direction of greatest descent.    
* fit - conducts the search until stopping conditions are met     

With respect to stopping criteria, we would want to stop the algorithm when one or more of the following conditions are met:    

* The number of iterations has reached a preset maximum,     
* The number of iterations has reached a preset minimum,     
* The relative or absolute change in the training set costs have fallen below a preset precision,     
* The relative or absolute change in the validation set costs have fallen below a preset precision,       
* The percent or absolute change in the gradient has fallen below a preset precision, and           
* The validation set costs has reached a preset maximum (in the case the algorithm starts to diverge).     

Lastly, since we will be building SGD and Minibatch Gradient Descent classes, we may want to create an abstract base class that contains the core functions used in the computation.

Ok, here we go.

#### Class Initialization
Let's initialize our abstract base class and concrete class for batch gradient descent. We may as well import some packages that we will need.
```{python class_init, echo=T, eval=F}
from abc import ABC, abstractmethod
import datetime
import math
import numpy as np
import pandas as pd


class GradientDescent(ABC):
    def __init__(self):
      pass
      
class BGD(GradientDescent):
    def __init__(self):
      pass    
```
`r kfigr::figr(label = "class_init", prefix = TRUE, link = TRUE, type="Figure")` Class Initialization

Now, lets define the base classes that will be inherited by the specific batch, stochastic, and mini-batch implementations.

#### Hypothesis Method
Recall that our linear hypothesis is:
$$h_\theta(x)=\theta_0x_0+\theta_1x_1=X^T \theta.$$
Thus our hypothesis method would simply be the dot product of our inputs $X^T$ and $\theta$. We can easily compute the dot product as follows.
```{python hypothesis_bgd, echo=T, eval=F}
def _hypothesis(self, X, theta):
        return(X.dot(theta))
```
`r kfigr::figr(label = "hypothesis_bgd", prefix = TRUE, link = TRUE, type="Figure")` Hypothesis Method

#### Error Method
Since we'll need the error for both the cost and gradient computation, let's create a method that implements:
$$e=h_\theta(x)-y$$
Taking the hypothesis from the previous method and y, we have:
```{python error_bgd, echo=T, eval=F}
def _error(self, h, y):
        return(h-y)
```
`r kfigr::figr(label = "error_bgd", prefix = TRUE, link = TRUE, type="Figure")` Error Method

#### Cost Method
Recall our cost function is:
$$J(\theta)=\frac{1}{2m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$$
Our method simply takes half of the mean squared error as follows:
```{python cost_bgd, echo=T, eval=F}
def _cost(self, e):
        return(1/2 * np.mean(e**2))
```
`r kfigr::figr(label = "cost_bgd", prefix = TRUE, link = TRUE, type="Figure")` Cost Method

#### Gradient Method
Next, we compute the gradient from:
$$
\nabla_\theta J =\frac{\partial}{\partial \theta_j}J(\theta)  = \frac{1}{m}\displaystyle\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})\cdot x^{(i)}_j \quad\quad \text{for every j}
$$
Note, $x_0^{(i)}$ for each $i$ is 1 - convenience to simplify our computation. 

This is a vector valued function. This means that it returns a vector of size $n$, where $n$ is the number of parameters in $X$. This the direction of steepest ascent in the objective function.
```{python gradient_bgd, echo=T, eval=F}
def _gradient(self, X, e):
        return(X.T.dot(e)/X.shape[0])
```
`r kfigr::figr(label = "gradient_bgd", prefix = TRUE, link = TRUE, type="Figure")` Gradient Method

#### Update Method
Our update function, you recall, looks like this.
$$\theta_j^{k+1} := \theta_j^k - \alpha \nabla_\theta J$$
Our update method is therefore:
```{python update_bgd, echo=T, eval=F}
def _update(self, alpha, theta, gradient):
        return(theta-(alpha * gradient))
```
`r kfigr::figr(label = "update_bgd", prefix = TRUE, link = TRUE, type="Figure")` Update Method

#### Finished Method
This method determines when the algorithm should stop. Again, we stop if the maximum number of iterations has been reached, or the absolute or relative change in our stopping measure has fallen below our precision parameter. If a minimum number of iterations is set, then the algorithm continues until that minimum is reached. We'll store the current state of the search in a dictionary.  It will contain the current iteration as well as the current and prior values of our evaluation parameter.

```{python, finished, echo=T, eval=F}
def _finished(self, state, miniter, maxiter, stop_metric, precision):
    if miniter:
        if miniter <= state['iteration']:
            if self._maxed_out(state['iteration']):
                return(True)
            elif stop_metric == 'a':
                return(abs(state['prior']-state['current']) < precision)
            else:
                return(abs(state['prior']-state['current'])/abs(state['prior']) < precision)     
    else:                    
        if self._maxed_out(state['iteration']):
            return(True)
        elif stop_metric == 'a':
            return(abs(state['prior']-state['current']) < precision)
        else:
            return(abs(state['prior']-state['current'])/abs(state['prior']) < precision)     

```
`r kfigr::figr(label = "finished_bgd", prefix = TRUE, link = TRUE, type="Figure")` Finished Method

we will also have a method that updates the state dictionary on each iteration. 
```{python, update_state, echo=T, eval=F}
def _update_state(self,state, iteration, J, J_val, g):
    state['iteration'] = iteration
    state['prior'] = state['current']
    if stop_parameter == 't':   # t for training Set Costs
        state['current'] = J
    elif stop_parameter == 'v': # v for validation set costs
        state['current'] = J_val
    else:                       # gradient
        state['current'] = np.sqrt(np.sum(abs(g)**2)) # Computes the norm of the gradient
    return(state)  

```
`r kfigr::figr(label = "update_state", prefix = TRUE, link = TRUE, type="Figure")` Update State

Now, we can define the fit method for our BGD class.

#### Fit Method
Finally, the workhorse of our algorithm. 
```{python fit_bgd, echo=T, eval=F}
class BGD(GradientDescent):
  def __init__(self):
      pass

  def fit(self, X, y, theta, X_val=None, y_val=None, 
             alpha=0.01, miniter=0, maxiter=0, precision=0.001,
             stop_measure='j', stop_metric='a', max_costs=100):
             
      state = {'iteration':0, 'prior':1, 'current':5}
      J_val = None
  
      # Set cross-validated flag if validation set included 
      cross_validated = all(v is not None for v in [X_val, y_val])
  
      while not self._finished(state, miniter, maxiter, stop_metric, precision):
          iteration += 1
  
          # Compute the costs and validation set error (if required)
          h = self._hypothesis(X, theta)
          e = self._error(h, y)
          J = self._cost(e)
          g = self._gradient(X, e)
  
          if cross_validated:
            h_val = self._hypothesis(X_val, theta)
            e_val = self._error(h_val, y_val)
            J_val = self._cost(e_val)                
            
          state = self._update_state(state, iteration, J, J_val, g)          
          
          theta = self._update(alpha, theta, g)
  
      d = dict()
      d['theta'] = theta
      d['J'] = J
      d['J_val'] = J_val
  
      return(d)
```
`r kfigr::figr(label = "fit_bgd", prefix = TRUE, link = TRUE, type="Figure")` Fit Method
Once we've initialized some variables, we iteratively:    

1. compute training set cost and gradient,   
2. compute validation set cost if we are using a cross-validation set,        
3. update our state dictionary, then    
4. perform the $\small\theta$ update.    

Once convergence criteria are met, we return our thetas and costs.

That's it, for the most part. The complete class definitions are available at https://github.com/DecisionScients/GradientDescent.

Next, we will learn how to set the hyperparameters that will determine algorithm performance.

### Hyperparameter Tuning via Gridsearch
The goal at this stage is to determine the set of hyperparameters that minimize cost on our validation set. A gridsearch approach will allow us to quickly sweep through a range of parameter values in an exhaustive search. Once we've completed the gridsearch, we will closely examine the best and most interesting solutions.

Our main hyperparameters are:      

* $\theta$: our initial thetas,      
* $\alpha$: our learning rate,    
* miniter : The minimum number of iterations to execute,     
* maxiter: The maximum number of iterations to execute,    
* max_costs: The maximum cost allowed, in case the algorithm diverges.         

Our convergence criteria parameters are:     

* stop parameter: The parameter we will use to evaluate *near* convergence. Values are:   
  - t: training set costs,    
  - v: validation set costs,  
  - g: gradient norm     
  
* stop metric: The metric we will use when evaluating the stop parameter. Values are:     
  - a: absolute change in the stop parameter, and      
  - r: relative change in the stop parameter.     
  
* precision: the $\epsilon$ for our stopping criteria, e.g. we stop when $J_k - J_{k-1} < \epsilon$

Our training and validation sets were randomly selected from our Ames Housing Project Training Set.  The training set is comprised of random sample of 500 observations. The remaining 960 samples make up the validation set. Our single predictor of sale price will be living area. Restricting our experiment to a single predictor will allow us to more easily visualize the algorithm's behavior. 

A small function was created to randomly sample the training data and split it into the appropriate training and validation sets.
```{python demo_data, echo=T, eval=F}
X, X_val, y, y_val = data.demo(n=500)
```

Now that we have our data, lets run our gridsearch.

#### GridSearch 
We will search a wide range of learning rates, precision parameters, and stopping conditions. Thus our parameters are:
```{python gs_params_1, echo=T, eval=F}
theta = np.array([-1,-1]) 
alpha = [0.02, 0.04, 0.06, 0.08, 0.1, 0.2, 0.4, 0.6, 0.8, 1]
precision = [0.1, 0.01, 0.001, 0.0001]
maxiter = 10000
stop_parameter = ['t', 'v', 'g']
stop_metric = ['a', 'r']
```
`r kfigr::figr(label = "gs_params_1", prefix = TRUE, link = TRUE, type="Figure")` Gridsearch Parameters

Ok, let's run our gridsearch.
```{python bgd_gs_echo, echo=T, eval=F}
lab = BGDLab()
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, alpha=alpha, precision=precision,
           maxiter=maxiter, stop_parameter=stop_parameter, stop_metric=stop_metric)
```

```{python bgd_gs, echo=F, eval=T}
report = bgd_gs()
```

### Analysis
`r kfigr::figr(label = "bgd_gs", prefix = TRUE, link = TRUE, type="Figure")` shows the validation set costs and elapsed time for `r nrow(py$report)` experiments by learning rate.

![](../report/figures/BGD/Validation Set Costs and TimeBy Learning Rate.png)
`r kfigr::figr(label = "bgd_gs", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Gridsearch

The solutions tend to cluster by learning rate.  The higher learning rates produce the best performance, from a cost and computation perspective. As we move out from the lower left corner of the plot we encounter declining performance as learning rates decrease. 

Let's take a closer look at the behavior and performance for a few learning rates, say 0.02, 0.1 and 0.8.

#### Search by Learning Rate
```{python bgd_featured, echo=F, eval=T}
alpha = [0.02, 0.1, 0.8]
featured = bgd_featured(report, alpha)
bgd_ani(featured, fontsize=24, cache=cache)
```

The animations in `r kfigr::figr(label = "bgd_search_by_alpha", prefix = TRUE, link = TRUE, type="Figure")` depict the path to convergence for learning rates 0.02, 0.1, and 0.8.  

<img src="../report/figures/BGD/Batch Gradient Descent Search - Alpha 0.02.gif" width="250" height="250" /><img src="../report/figures/BGD/Batch Gradient Descent Search - Alpha 0.1.gif" width="250" height="250" /><img src="../report/figures/BGD/Batch Gradient Descent Search - Alpha 0.8.gif" width="250" height="250" />

`r kfigr::figr(label = "bgd_search_by_alpha", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Search by Learning Rate

As expected, fastest convergence is associated with higher learning rates. So, how do these solutions fit the data?

#### Fit by Learning Rate
Not very well. `r kfigr::figr(label = "bgd_fit_by_alpha", prefix = TRUE, link = TRUE, type="Figure")` illustrates the regression line fit for the search. Neither of the solutions fit the data very well.  What's happening?

<img src="../report/figures/BGD/Batch Gradient Descent Fit - Alpha 0.02.gif" width="250" height="250" /><img src="../report/figures/BGD/Batch Gradient Descent Fit - Alpha 0.1.gif" width="250" height="250" /><img src="../report/figures/BGD/Batch Gradient Descent Fit - Alpha 0.8.gif" width="250" height="250" />

`r kfigr::figr(label = "bgd_fit_by_alpha", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Fit by Learning Rate

Notice, the left and center solutions have gradient based stopping criteria. In these cases, we are observing a *vanishing gradient*. As the solution approaches convergence, the gradient becomes vanishingly small, effectively preventing the $\theta$'s from changing their values. Hence the change in the gradient, from iteration to iteration, vanishes below the $\epsilon$ and the algorithm stops. This extends to other measures such as training set or validation set costs. As the $\theta$'s converge, the change in training and validation set costs diminishes falls below $\epsilon$ and the algorithm stops.

We will need to introduce another parameter to our stopping condition. We will force the algorithm to execute a minimum number of iterations, after which, we will evaluate our gradient, and/or cost based stopping criteria.

#### Search by Learning Rate, Minimum Iterations = 100
```{python bgd_feature_miniter, echo=F, eval=T}
alpha = [0.02, 0.1, 0.8]
featured = bgd_featured(report, alpha)
bgd_ani(featured, miniter=100, fontsize=24, cache=cache)
```

`r kfigr::figr(label = "bgd_search_by_alpha_100", prefix = TRUE, link = TRUE, type="Figure")` shows the convergence path for learning rates 0.02, 0.1,and 0.8 for a minimum of 100 iterations. 

<img src="../report/figures/BGD/Batch Gradient Descent Search - Alpha 0.02 Minimum Iterations - 100.gif" width="250" height="250" /><img src="../report/figures/BGD/Batch Gradient Descent Search - Alpha 0.1 Minimum Iterations - 100.gif" width="250" height="250" /><img src="../report/figures/BGD/Batch Gradient Descent Search - Alpha 0.8 Minimum Iterations - 100.gif" width="250" height="250" />

`r kfigr::figr(label = "bgd_search_by_alpha_100", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Search by Learning Rate - Minimum Iterations = 100

In each case, we see a longer path to convergence relative to those in `r kfigr::figr(label = "bgd_search_by_alpha", prefix = TRUE, link = TRUE, type="Figure")`. Learning rate 0.02 achieves a validation set cost of approximately 0.06 in 100 iterations. Increasing the learning rate to 0.1 reduces validation cost to about 0.03. We also see an interesting curve in the optimization path.  At about the 30th iteration, the gradient turns in the direction of the slope axis. Lastly, a learning rate of 0.8 produces validation set costs of about 0.02. Again, the gradient makes a turn in the direction of the slope early in the optimization process.  The majority of the cycles were spent optimizing in the slope direction.

Let's see how these solutions fit the data.

#### Fit by Learning Rate, Minimum Iterations = 100
The regression lines in `r kfigr::figr(label = "bgd_fit_by_alpha_100", prefix = TRUE, link = TRUE, type="Figure")` show a marked improvement over those in `r kfigr::figr(label = "bgd_fit_by_alpha", prefix = TRUE, link = TRUE, type="Figure")`. 

<img src="../report/figures/BGD/Batch Gradient Descent Fit - Alpha 0.02 Minimum Iterations - 100.gif" width="250" height="250" /><img src="../report/figures/BGD/Batch Gradient Descent Fit - Alpha 0.1 Minimum Iterations - 100.gif" width="250" height="250" /><img src="../report/figures/BGD/Batch Gradient Descent Fit - Alpha 0.8 Minimum Iterations - 100.gif" width="250" height="250" />

`r kfigr::figr(label = "bgd_fit_by_alpha_100", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Fit by Learning Rate - Minimum Iterations = 100

Here we see the effect of the learning rate when the number of iterations is fixed. Learning rate 0.02 was too small to make adequate progress within 100 iterations.  The situation was slightly better for learning rate 0.1. But our best result was with $\alpha=0.8$. We were able to obtain a reasonably good fit within 100 iterations. 

### Key Take-Aways
Let's recap the key points for batch gradient descent.   

* The advantage of batch gradient descent vis-a-vis the other variants is the stability of the gradient and the quality of the gradient approximation.   
* The disadvantages are:   
  - The stable gradient may result in a suboptimal solution as batch gradient descent is susceptible to saddle points and local minima.   
  - Convergence may be slow, especially for large data sets.    
  - Memory intensive as the entire dataset must be memory resident.     
  - In 'classic' batch gradient descent, learning rates are fixed and do not adjust to contours of the objective function.      
* Learning rates determine not only step size, but convergence rate as well. Smaller learning rates tend to converge at slower rates.   
* As a solution approaches convergence, the gradient becomes vanishingly small, effectively preventing the $\theta$'s from changing their values.  Consequently, changes in the gradient and objective function costs diminish to values approaching zero and the algorithm stops.     
* One technique for dealing with vanishing gradient is to force the algorithm to process a preset minimum number of iterations. This hyperparameter can be set based upon available computation resources and time.               

In the next section, we will update our GradientDescent module to support stochastic gradient descent.
