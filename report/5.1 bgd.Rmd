<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return ''+n}
}
}});</script>

```{python GradientDescent, code=readLines('../src/GradientDescent.py'), message=FALSE, warning=FALSE}
```
```{python GradientDemo, code=readLines('../src/GradientDemo.py'), message=FALSE, warning=FALSE}
```
```{python GradientFit, code=readLines('../src/GradientFit.py'), message=FALSE, warning=FALSE}
```
```{python GradientLab, code=readLines('../src/GradientLab.py'), message=FALSE, warning=FALSE}
```

```{python defaults, echo=F, eval=T}
directory = "./report/figures/BGD/Lab"
```

## Batch Gradient Descent 
Batch gradient descent updates the parameters $\theta$ after computing the gradient on the entire training set. This is the key difference between batch gradient descent and its variants. 

The advantage of computing the gradient on the entire dataset is that it produces a better, more stable approximation of the gradient. 

On the other hand, batch gradient descent has its challenges:    

* The stability of the gradient may result in a suboptimal state of convergence when local minimums or saddle points are encountered. A more noisy gradient can enable the algorithm to escape saddle points or subvert local minima.     
* Computing the gradient on the entire dataset often results in a slower convergence, especially for large datasets.    
* Batch gradient descent requires the entire dataset to be memory resident, which can be problematic for large datasets.   
* Classic batch gradient descent works on a constant learning rate for all iterations of the search. Ideally, we would like a learning rate to adapt to the contours of our objective function. Many textbook optimization algorithms use analytic approaches, such as Newton's method, to determine the exact step size. However, the computation can be too expensive to be used in the context of large neural networks. Most practitioners simply fix it to a constant or slowly decay it over iterations. Determining optimal learning rates is an area of open research.

In the following sections, we will review the algorithm, implement a basic batch gradient descent class, perform hyperparameter tuning using a gridsearch, then evaluate algorithm behavior for select hyperparameter sets. 

### Algorithm
Batch gradient descent, starts with an initial $\theta$ and performs the following recursive update rule:
$$\theta_j^{k+1} := \theta_j^k - \alpha \nabla_\theta J,$$
for $k$ iterations until approximate convergence condition or a preset maximum number of iterations is reached.
The pseudocode for the algorithm looks like this. 
```{r eval=F, echo=T, tidy=F}
Standardize our data
Initialize hyperparameters  
While stopping condition is not met 
  h = predict(data, thetas)
  c = costs(h, y, data)
  g = gradient(costs, data)
  thetas = update(thetas, learning_rate, gradient)
return (thetas)
```
`r kfigr::figr(label = "gd_algorithm", prefix = TRUE, link = TRUE, type="Code")`: Gradient Descent Algorithm

### Implementation
Now, we will build the batch gradient descent class that will be used to solve our challenge problem. Before we start getting our code on, let's establish a few design considerations.

#### Design Considerations   
Our GradientDescent class will need the following methods:    

* hypothesis - computes the hypothesis given $X$ and the parameters, $\theta$.     
* cost - computes the cost given $y$ and our hypotheses.     
* gradient - calculates the gradient based upon error and the values $X_j$ for each $j$.      
* update - computes the updates to the $\theta$'s in the direction of greatest descent.    
* fit - conducts the search until stopping conditions are met     

With respect to stopping criteria, we would want to stop the algorithm when one or more of the following conditions are met:    

* The number of iterations has reached a preset maximum,     
* The convergence hasn't improved in $\iota$ iterations in a row,       

Convergence is checked once per epoch against a validation set (if provided), or the training set. The improvement in convergence is evaluated with a precision parameter $\epsilon$.

Lastly, since we will be building SGD and Minibatch Gradient Descent classes, we may want to create an abstract base class that contains the core functions used in the computation.

Ok, here we go.

#### Class Initialization
Let's initialize our abstract base class and concrete class for batch gradient descent. We may as well import some packages that we will need.
```{python class_init, echo=T, eval=F}
from abc import ABC, abstractmethod
import datetime
import math
import numpy as np
import pandas as pd


class GradientDescent(ABC):
    def __init__(self):
      pass
      
class BGD(GradientDescent):
    def __init__(self):
      pass    
```
`r kfigr::figr(label = "class_init", prefix = TRUE, link = TRUE, type="Code")` Class Initialization

Now, lets define the base classes that will be inherited by the specific batch, stochastic, and mini-batch implementations.

#### Hypothesis Method
Recall that our linear hypothesis is:
$$h_\theta(x)=\theta_0x_0+\theta_1x_1=X^T \theta.$$
Thus our hypothesis method would simply be the dot product of our inputs $X^T$ and $\theta$. We can easily compute the dot product as follows.
```{python hypothesis_bgd, echo=T, eval=F}
def _hypothesis(self, X, theta):
        return(X.dot(theta))
```
`r kfigr::figr(label = "hypothesis_bgd", prefix = TRUE, link = TRUE, type="Code")` Hypothesis Method

#### Error Method
Since we'll need the error for both the cost and gradient computation, let's create a method that implements:
$$e=h_\theta(x)-y$$
Taking the hypothesis from the previous method and y, we have:
```{python error_bgd, echo=T, eval=F}
def _error(self, h, y):
        return(h-y)
```
`r kfigr::figr(label = "error_bgd", prefix = TRUE, link = TRUE, type="Code")` Error Method

#### Cost Method
Recall our cost function is:
$$J(\theta)=\frac{1}{2m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$$
Our method simply takes half of the mean squared error as follows:
```{python cost_bgd, echo=T, eval=F}
def _cost(self, e):
        return(1/2 * np.mean(e**2))
```
`r kfigr::figr(label = "cost_bgd", prefix = TRUE, link = TRUE, type="Code")` Cost Method

#### Gradient Method
Next, we compute the gradient from:
$$
\nabla_\theta J =\frac{\partial}{\partial \theta_j}J(\theta)  = \frac{1}{m}\displaystyle\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})\cdot x^{(i)}_j \quad\quad \text{for every j}
$$
Note, $x_0^{(i)}$ for each $i$ is 1 - convenience to simplify our computation. 

This is a vector valued function. This means that it returns a vector of size $n$, where $n$ is the number of parameters in $X$. This the direction of steepest ascent in the objective function.
```{python gradient_bgd, echo=T, eval=F}
def _gradient(self, X, e):
        return(X.T.dot(e)/X.shape[0])
```
`r kfigr::figr(label = "gradient_bgd", prefix = TRUE, link = TRUE, type="Code")` Gradient Method

#### Update Method
Our update function, you recall, looks like this.
$$\theta_j^{k+1} := \theta_j^k - \alpha \nabla_\theta J$$
Our update method is therefore:
```{python update_bgd, echo=T, eval=F}
def _update(self, alpha, theta, gradient):
        return(theta-(alpha * gradient))
```
`r kfigr::figr(label = "update_bgd", prefix = TRUE, link = TRUE, type="Code")` Update Method

#### Finished Method
This method determines when the algorithm should stop. Again, we stop if the maximum number of iterations has been reached, or the absolute or relative change in our stopping measure has fallen below our precision parameter. If a minimum number of iterations is set, then the algorithm continues until that minimum is reached. We'll store the current state of the search in a dictionary.  It will contain the current iteration as well as the current and prior values of our evaluation parameter.

```{python, finished, echo=T, eval=F}
def _finished(self, state, miniter, maxiter, stop_metric, precision):
    if miniter:
        if miniter <= state['iteration']:
            if self._maxed_out(state['iteration']):
                return(True)
            elif stop_metric == 'a':
                return(abs(state['prior']-state['current']) < precision)
            else:
                return(abs(state['prior']-state['current'])/abs(state['prior']) < precision)     
    else:                    
        if self._maxed_out(state['iteration']):
            return(True)
        elif stop_metric == 'a':
            return(abs(state['prior']-state['current']) < precision)
        else:
            return(abs(state['prior']-state['current'])/abs(state['prior']) < precision)     

```
`r kfigr::figr(label = "finished_bgd", prefix = TRUE, link = TRUE, type="Code")` Finished Method

we will also have a method that updates the state dictionary on each iteration. 
```{python, update_state, echo=T, eval=F}
def _update_state(self,state, iteration, J, J_val, g):
    state['iteration'] = iteration
    state['prior'] = state['current']
    if stop_parameter == 't':   # t for training Set Costs
        state['current'] = J
    elif stop_parameter == 'v': # v for validation set costs
        state['current'] = J_val
    else:                       # gradient
        state['current'] = np.sqrt(np.sum(abs(g)**2)) # Computes the norm of the gradient
    return(state)  

```
`r kfigr::figr(label = "update_state", prefix = TRUE, link = TRUE, type="Code")` Update State

Now, we can define the fit method for our BGD class.

#### Fit Method
Finally, the workhorse of our algorithm. 
```{python fit_bgd, echo=T, eval=F}
class BGD(GradientDescent):
  def __init__(self):
      pass

  def fit(self, X, y, theta, X_val=None, y_val=None, 
             alpha=0.01, miniter=0, maxiter=0, precision=0.001,
             stop_measure='j', stop_metric='a', max_costs=100):
             
      state = {'iteration':0, 'prior':1, 'current':5}
      J_val = None
  
      # Set cross-validated flag if validation set included 
      cross_validated = all(v is not None for v in [X_val, y_val])
  
      while not self._finished(state, miniter, maxiter, stop_metric, precision):
          iteration += 1
  
          # Compute the costs and validation set error (if required)
          h = self._hypothesis(X, theta)
          e = self._error(h, y)
          J = self._cost(e)
          g = self._gradient(X, e)
  
          if cross_validated:
            h_val = self._hypothesis(X_val, theta)
            e_val = self._error(h_val, y_val)
            J_val = self._cost(e_val)                
            
          state = self._update_state(state, iteration, J, J_val, g)          
          
          theta = self._update(alpha, theta, g)
  
      d = dict()
      d['theta'] = theta
      d['J'] = J
      d['J_val'] = J_val
  
      return(d)
```
`r kfigr::figr(label = "fit_bgd", prefix = TRUE, link = TRUE, type="Code")` Fit Method
Once we've initialized some variables, we iteratively:    

1. compute training set cost and gradient,   
2. compute validation set cost if we are using a cross-validation set,        
3. update our state dictionary, then    
4. perform the $\small\theta$ update.    

Once convergence criteria are met, we return our thetas and costs.

That's it, for the most part. The complete class definitions are available at https://github.com/DecisionScients/GradientDescent.

Next, we will learn how to set the hyperparameters that will determine algorithm performance.

### Tuning Batch Gradient Descent
Batch gradient descent is parameterized most essentially by the learning rate, $\alpha$. In this section, we will use a gridsearch technique to explore the algorithms behavior for a range of learning rates. Our implementation is also parameterized by:    
* $\theta$: Our initial guess at our parameters     
* $i_{max}$: The maximum number of iterations to execute,    
* $i_{stop}$: The number of consecutive iterations of 'non-improvement' required to stop the algorithm.   
* $\epsilon$: The tolerance or precision parameter with which objective function improvement will be evaluated.   

To isolate learning rate effects, we'll fix these *secondary* parameters as follows.
```{python bgd_gs_params, echo=T, eval=T}
theta = np.array([-1,-1]) 
alpha = [0.02, 0.04, 0.06, 0.08, 0.1, 0.2, 0.4, 0.6, 0.8, 1]
precision = [0.001]
improvement = [5]
maxiter = 10000
```
`r kfigr::figr(label = "bgd_gs_params", prefix = TRUE, link = TRUE, type="Code")` Gridsearch Parameters

#### The Data
Our training and validation sets were randomly selected from our Ames Housing Project Training Set.  The training set is comprised of random sample of 500 observations. The remaining 960 samples make up the validation set. Our single predictor of sale price will be living area. Restricting our experiment to a single predictor will allow us to more easily visualize the algorithm's behavior. 

So, let's grab our data. We have a small function that was created to randomly sample the training data and split it into the appropriate training and validation sets.
```{python bgd_gs_data, echo=T, eval=T}
X, X_val, y, y_val = data.demo(n=500)
```
`r kfigr::figr(label = "bgd_gs_data", prefix = TRUE, link = TRUE, type="Code")` Batch Gradient Descent Gridsearch Data

Gradient descent is sensitive to feature scaling so let's scale our data to the range [0,1] using sklearn's preprocessing module.
```{python bgd_scale, echo=T, eval=F}
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X = scaler.fit_transform(X)
X_val = scaler.fit_transform(X_val)
y = scaler.fit_transform(y)
y_val = scaler.fit_transform(y_val)
```
`r kfigr::figr(label = "bgd_scale", prefix = TRUE, link = TRUE, type="Code")` Batch Gradient Descent Gridsearch Data Scaling

Now that we have our scaled data, lets run our gridsearch.

#### GridSearch 
A class,  BGDLab, was created to perform our gridsearch. Let's instantiate the class, run the gridsearch and obtain some diagnostic information. 
```{python bgd_lab, echo=T, eval=T}
lab = BGDLab()
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, alpha=alpha, precision=precision,
               maxiter=maxiter, improvement=improvement)
bgd_summary = lab.summary()
bgd_detail = lab.get_detail()   
bgd_report_filename='Batch Gradient Descent Gridsearch Report.csv'
bgd_report = lab.report(directory=directory, filename=bgd_report_filename)
```
`r kfigr::figr(label = "bgd_lab", prefix = TRUE, link = TRUE, type="Code")` Batch Gradient Descent Gridsearch 

#### Learning Rate Analysis
As a first step in our analysis, let's examine the speed and accuracy with which each learning rate converges. 

```{python bgd_learning_curves, echo=F, eval=T}
lab.figure(data=bgd_detail, x='iterations', y='cost', z='alpha', 
           func=lab.lineplot, directory=directory, height=2)   
```

![](../report/figures/BGD/Lab/Batch Gradient DescentTraining Set Costs By Iterations and Learning Rate.png)

`r kfigr::figr(label = "bgd_learning_curve_plot", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Learning Curves

In general, the algorithm is behaving as expected. Higher learning rates correlated with faster convergence times and and lower training set costs. Learning rates of 0.6 and above appear to produce the best training set cost results witin about 10 iterations. 

Let's take a closer look at final training set costs and computation times.

```{python bgd_alpha_costs, echo=F, eval=T}
lab.figure(data=bgd_summary, x='alpha', y='final_costs',
           func=lab.barplot, directory=directory, width=0.5)   
lab.figure(data=bgd_summary, x='alpha', y='duration',
           func=lab.barplot, directory=directory, width=0.5)              
```

![](../report/figures/BGD/Lab/Batch Gradient DescentTraining Set Costs By Learning Rate.png){width=50%}![](../report/figures/BGD/Lab/Batch Gradient DescentComputation Time (ms) By Learning Rate.png){width=50%}

`r kfigr::figr(label = "bgd_cost_time", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Quality and Training Time
```{r, bgd_best, echo=F, eval=T}
bgd_report = read.csv(file.path(py$directory, py$bgd_report_filename))
bgd_best = bgd_report[1,]
```

Indeed, larger learning rates within our parameter space performed better from both a quality and time perspective. In fact, our best performing learning rate was actually `r bgd_best['alpha']`. It produced a validation set final cost of `r bgd_best['final_costs_val']`.

Let's evaluate the convergence and fit of this solution.

#### Evaluation
The `r kfigr::figr(label = "bgd_converge", prefix = TRUE, link = TRUE, type="Figure")` visualization reveals the path of the $\theta$'s to approximate convergence. The oscillation around the minimum is emblematic of high learning rate gradient descent. Approximate convergence was obtained in 7 iterations, presenting a final training set cost $J\approx0.03$.
```{python bgd_demo, echo=F, eval=T}
alpha = bgd_report.iloc[0]['alpha']
precision = bgd_report.iloc[0]['precision']
improvement = bgd_report.iloc[0]['improvement']

bgd = BGDDemo()
bgd.fit(X=X, y=y, theta=theta, X_val=X_val, y_val=y_val, alpha=alpha, precision=precision,
        maxiter=maxiter, improvement=improvement)
bgd.show_search(directory=directory, fps=1)
bgd.show_fit(directory=directory, fps=1)
```

![](../report/figures/BGD/Lab/Batch Gradient Descent Search Plot Learning Rate 1.0.gif)

`r kfigr::figr(label = "bgd_converge", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent - Alpha = 1.0

So, how does the solution fit the training data? Great question!

The `r kfigr::figr(label = "bgd_fit", prefix = TRUE, link = TRUE, type="Figure")` visualization illustrates the regression line fit vis-a-vis the 500 observations in our training set.   

![](../report/figures/BGD/Lab/Batch Gradient Descent Fit Plot Learning Rate 1.0.gif)

`r kfigr::figr(label = "bgd_fit", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Fit

Actually, the solution doesn't present a very good fit to the data. What's happening here?

The challenge with this optimization problem is that of the *vanishing gradient*. This was intentionally introduced by setting the $\small \theta =[-1,-1]$. Since we assume home prices increase with area, the optimal solution would have a positive slope. Similarly, we don't have negative home prices, so the intercept would likely be positive as well. Setting the $\small \theta =[-1,-1]$ would *force* the gradient descent through a sign change.

Note the gradients (slope of the regression lines) in `r kfigr::figr(label = "bgd_fit", prefix = TRUE, link = TRUE, type="Figure")`. It starts with a *negative* slope, then slowly increases towards the point of zero slope, a.k.a. the point of the *vanishing gradient*. Since the step size is scaled by the magnitude of the gradient, the size of each step diminishes with gradient. Hence, the corresponding change in the cost function decays, ultimately to a value less than $\epsilon$ and the search stops. 

#### Refinement
So, how do we remedy this situation? We can fix this by reducing our precision parameter, $\epsilon$, increasing the improvement parameter $i_{stop}$, or both. Let's rerun our gradient search, but this time, we'll expand our parameter space to include a range of $\epsilon$ and $i_{stop}$ values.  

```{python bgd_gs_params_2, echo=T, eval=T}
theta = np.array([-1,-1]) 
alpha = [0.02, 0.04, 0.06, 0.08, 0.1, 0.2, 0.4, 0.6, 0.8, 1]
precision = [0.001, 0.0001]
improvement = [5,10,15,20]
maxiter = 10000
```
`r kfigr::figr(label = "bgd_gs_params_2", prefix = TRUE, link = TRUE, type="Code")`: Gridsearch II Parameters

Ok, let's rerun our gridsearch and extract 

```{python bgd_lab_II, echo=T, eval=T}
lab = BGDLab()
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, alpha=alpha, precision=precision,
               maxiter=maxiter, improvement=improvement)
bgd_report_filename='Batch Gradient Descent Gridsearch II Report.csv'
bgd_report = lab.report(directory=directory, filename=bgd_report_filename)
```
`r kfigr::figr(label = "bgd_lab_II", prefix = TRUE, link = TRUE, type="Code")`: Batch Gradient Descent Gridsearch II


```{r, bgd_best_alpha, echo=F, eval=T}
bgd_report = read.csv(file.path(py$directory, py$bgd_report_filename))
bgd_best = bgd_report %>% group_by(alpha) %>% slice(which.min(final_costs_val))
rownames(bgd_best) <- NULL
bgd_best['X'] <- NULL
```

`r kfigr::figr(label = "bgd_best_alpha_tbl", prefix = TRUE, link = TRUE, type="Table")` lists the best performing solutions by learning rate for `r nrow(bgd_report)` searches.

`r kfigr::figr(label = "bgd_best_alpha_tbl", prefix = TRUE, link = TRUE, type="Table")`: Batch Gradient Descent Gridsearch II Best Performing Solutions by Learning Rate
```{r, bgd_best_alpha_tbl, echo=F, eval=T}
knitr::kable(bgd_best, digits = 4, format.args = list(big.mark = ",")) %>%
 kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center")
```

Once again, we observe the best performance from a quality and computation perspective with learning rate $\alpha=1.0$. By increasing our improvement parameter, $i_{stop}$ to 20 iterations, we've reduced validation set costs by nearly half, from $J\approx0.03$ to $J\approx0.017$.

```{python bgd_demo_II, echo=F, eval=T}
alpha = bgd_report.iloc[0]['alpha']
precision = bgd_report.iloc[0]['precision']
improvement = bgd_report.iloc[0]['improvement']
bgd_search_filename = 'Batch Gradient Descent - Gridsearch II.gif'
bgd_fit_filename = 'Batch Gradient Descent - Gridsearch II - Fit.gif'

bgd = BGDDemo()
bgd.fit(X=X, y=y, theta=theta, X_val=X_val, y_val=y_val, alpha=alpha, precision=precision,
        maxiter=maxiter, improvement=improvement)
bgd.show_search(directory=directory, filename=bgd_search_filename, fps=1)
bgd.show_fit(directory=directory, filename=bgd_fit_filename, fps=1)
```

![](../report/figures/BGD/Lab/Batch Gradient Descent - Gridsearch II.gif)

`r kfigr::figr(label = "bgd_converge_II", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent II

![](../report/figures/BGD/Lab/Batch Gradient Descent - Gridsearch II - Fit.gif)

`r kfigr::figr(label = "bgd_fit_II", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent II Fit