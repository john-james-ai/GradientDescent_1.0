<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return ''+n}
}
}});</script>

#### Time Decay Learning Rates
Our purpose here is to identify the time decay learning rate hyperparameters that minimize our objective function on our training set. Recall that, time decay based learning rates adjust according to the number of iterations. Our time decay implementation is mathematically defined as:     
$$\alpha=\alpha_0/(1+kt)$$
where:     

* $\alpha$ is the new learning rate,   
* $\alpha_0$ is the initial learning rate,      
* $t$ is the iteration number,    
* $k$ is the decay hyperparameter.

##### Time Decay Learning Rate Gridsearch 
For our initial alpha, we will use $\alpha_0=1.6$, the maximum learning rate from the learning rate test in `r kfigr::figr(label = "bgd_constant_alpha_plots_i", prefix = TRUE, link = TRUE, type="Figure")`. Using a gridsearch, we will evaluate a range of values for $k$. Again, we'll use default values for our initial $\theta$'s, maxiter, and precision $\epsilon$ parameters. Learning from the last section, we will set the $i_s$ parameter to 20 to grant the algorithm the time to arrive at a suitable optimum. 

```{python bgd_time_params_i, echo=T, eval=T}
theta = np.array([-1,-1]) 
learning_rate_sched = 't'
learning_rate = [1.6]
time_decay = np.arange(0.01,1,0.01)
precision = [0.001]
maxiter = [5000]
i_s=[20]
```

Ok, let's run the search and evaluate performance. 

```{python bgd_time_gs, echo=T, eval=T}
lab = BGDLab()
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, learning_rate=learning_rate, 
               learning_rate_sched=learning_rate_sched, time_decay=time_decay, 
               step_decay=None, step_epochs=None, exp_decay=None,
               precision=precision, maxiter=maxiter, i_s=i_s)
```

##### Time Decay Learning Rate Analysis
How does time decay parameter affect objective function performance and computation times? The following visually represents training set costs and computation times vis-a-vis our time decay parameter.

```{python bgd_time_plots, echo=F, eval=T}
filename = 'Batch Gradient Descent - Time Decay - Costs by Learning Rate.png'
lab.figure(data=lab.summary(), x='time_decay', y='final_costs', 
           func=lab.lineplot, directory=directory, 
           filename=filename, width=0.5)             
filename = 'Batch Gradient Descent - Time Decay - Time by Learning Rate.png'
lab.figure(data=lab.summary(), x='time_decay', y='duration', 
           func=lab.lineplot, directory=directory, 
           filename=filename, width=0.5)                        
filename = 'Batch Gradient Descent - Time Decay Report.csv'             
bgd_time_decay_report = lab.report(directory=directory, filename=filename)
```

![](../report/figures/BGD/Lab/Batch Gradient Descent - Time Decay - Costs by Learning Rate.png){width=50%}![](../report/figures/BGD/Lab/Batch Gradient Descent - Time Decay - Time by Learning Rate.png){width=50%}

`r kfigr::figr(label = "bgd_time_plots", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Training Costs and Computation Time by Time Decay Parameter

```{r bgd_time_report, echo=F, eval=T}
report <- read.csv(file.path(py$directory, py$filename))
report_costs <- report %>% arrange(final_costs, duration) %>% select(learning_rate, time_decay, final_costs, duration)
report_error <- report %>% arrange(final_mse, duration) %>% select(learning_rate, time_decay, final_mse)
```

As indicated in `r kfigr::figr(label = "bgd_time_plots", prefix = TRUE, link = TRUE, type="Figure")`, higher values of the time decay correlate with greater training set costs. Computation times, on the other hand, appear to be inversely correlated with our time decay parameter $k$. By extension, training set costs are inversely correlated with computation time. 

That said, which values of our time decay parameter, $k$, yield best objective function performance? The following table lists the top performing parameter sets.

`r kfigr::figr(label = "bgd_best_time", prefix = TRUE, link = TRUE, type="Table")`: Batch Gradient Descent Best Performing Time Decay Learning Rates
```{r bgd_best_time}
knitr::kable(head(report_costs, 3), format.args = list(big.mark = ",")) %>%
 kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = F, position = "center")
```

As indicated in `r kfigr::figr(label = "bgd_best_time", prefix = TRUE, link = TRUE, type="Table")`, we obtain a minimum cost, $J\approx$ `r round(report_costs['final_costs'][1,],3)` in approximately `r round(report_costs['duration'][1,],3)` milliseconds, with an initial learning rate, $\alpha_0=$ `r report_costs['learning_rate'][1,]` and time decay parameter $k=$ `r report_costs['time_decay'][1,]`.

Next, let's examine the solution path for our best performing hyperparameter set.

##### Time Decay Learning Rate Solution Path
The following animation reveals the effect of a time decayed learning rate over 27 iterations, starting with an initial learning rate $\alpha=1.6$. 
```{python bgd_time_params, echo=F, eval=T}
theta = np.array([-1,-1]) 
learning_rate_sched = bgd_time_decay_report['learning_rate_sched'].iloc[0]
learning_rate = bgd_time_decay_report['learning_rate'].iloc[0]
time_decay = bgd_time_decay_report['time_decay'].iloc[0]
precision = bgd_time_decay_report['precision'].iloc[0]
maxiter = bgd_time_decay_report['maxiter'].iloc[0]
i_s= bgd_time_decay_report['i_s'].iloc[0]
```


```{python bgd_time_path, echo=F, eval=T, cache=T}
time_decay_demo = BGDDemo()
time_decay_demo.fit(X=X,y=y, theta=theta, learning_rate=learning_rate, 
         learning_rate_sched=learning_rate_sched, time_decay=time_decay,
         precision=precision, maxiter=maxiter, 
         i_s=i_s)
         
filename = 'Batch Gradient Descent - Time Decay Convergence.gif'
time_decay_demo.show_search(directory=directory, filename=filename, fps=1)
filename = 'Batch Gradient Descent - Time Decay Fit.gif'
time_decay_demo.show_fit(directory=directory, filename=filename, fps=1)
```
![](../report/figures/BGD/Lab/Batch Gradient Descent - Time Decay Convergence.gif)

`r kfigr::figr(label = "bgd_time_path", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Time Decay Convergence k=0.01

The oscillation in the solution path, caused by the very high initial learning rate $\alpha=1.6$, diminishes systematically as the learning rate decays. Optimal cost of $J\approx$ `r round(report_costs['final_costs'][1,],3)` is reached in 27 iterations.  `r kfigr::figr(label = "bgd_time_alpha", prefix = TRUE, link = TRUE, type="Figure")` reveals the learning rates applied at each iteration. 

```{python bgd_time_alpha, echo=F, eval=T}
filename = 'Batch Gradient Descent - Time Decay - Learning Rates by Iteration.png'
data = lab.detail()
data_plot = data[data.time_decay==bgd_time_decay_report['time_decay'].iloc[0]]
lab.figure(data=data_plot, x='iterations', y='learning_rates', 
           func=lab.lineplot, directory=directory, 
           filename=filename, width=1)
last = data['learning_rates'].iloc[-1].item()
```
![](../report/figures/BGD/Lab/Batch Gradient Descent - Time Decay - Learning Rates by Iteration.png)

`r kfigr::figr(label = "bgd_time_alpha", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent - Time Decay Learning Rates by Iteration

##### Time Decay Learning Rate Evaluation
Here, we evaluate the empirical solution obtained from the training set as well as the generalizability of the solution to new data. We will use regression line fit to assess our empirical result. Validation set performance will be our measure of generalizability.   

![](../report/figures/BGD/Lab/Batch Gradient Descent - Time Decay Fit.gif)

`r kfigr::figr(label = "bgd_time_fit", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Time Decay Fit k=0.01

The time decay effect is evident in the regression plot above As a consequence of the high initial learning rate $\alpha=$ `r report_costs['learning_rate'][1,]`, the oscillation around the optimum is rather dramatic at first. The oscillations diminish in magnitude as the learning rate decays and the norm of the gradient goes to zero. Optimal parameters $\theta$ are obtained in 27 iterations at a minimum cost of $J\approx$ `r round(report_costs['final_costs'][1,],3)`

Does our solution generalize well to unseen data? If it did, we would expect that our best performing parameters on the training set would be among the best performing parameters on the validation set. The following plot contrasts training and validation set performance across our parameter sets.

```{python bgd_time_eval, echo=F, eval=T}
filename = 'Batch Gradient Descent Time Decay Learning Rate Costs.png'
lab.figure(data=lab.summary(), x='time_decay', y='final_costs',
           func=lab.lineplot, directory=directory, 
           filename=filename, width=0.5)  
filename = 'Batch Gradient Descent Time Decay Learning Rate Error.png'
lab.figure(data=lab.summary(), x='time_decay', y='final_mse',
           func=lab.lineplot, directory=directory, 
           filename=filename, width=0.5)         
```
![](../report/figures/BGD/Lab/Batch Gradient Descent Time Decay Learning Rate Costs.png){width=50%}![](../report/figures/BGD/Lab/Batch Gradient Descent Time Decay Learning Rate Error.png){width=50%}

`r kfigr::figr(label = "bgd_time_alpha", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Time Decay Learning Rate Costs and Error

The plots look remarkably similar. In fact, both the minimum training set cost $J=$ `r round(report_costs['final_costs'][1,], 4)` and the minimum validation set error, `r round(report_error['final_mse'][1,], 4)`, occur at a time decay value $k=$ `r report_error['time_decay'][1,]`. That said, we can conclude that the solution generalizes reasonably well to unseen data.

##### Time Decay Learning Rate Solution Summary
To summarize, we obtained an optimal solution of $J\approx$ `r round(report_costs['final_costs'][1,], 4)`, within about `r round(report_costs['duration'][1,],3)` milliseconds with the following parameters:

`r kfigr::figr(label = "bgd_time_decay_summary_params", prefix = TRUE, link = TRUE, type="Table")`: Batch Gradient Descent Time Decay Learning Rate Solution Hyperparameters
```{r bgd_time_decay_summary_params}
params <- report %>% select(learning_rate, time_decay, precision, maxiter, i_s) 
knitr::kable(t(params[1,]), format.args = list(big.mark = ",")) %>%
 kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = F, position = "center")
```

The optimal results, in terms of training set costs and validation set error are:
```{python bgd_time_decay_summary_results, echo=F, eval=T}
bgd = BGD()
bgd.fit(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, learning_rate=learning_rate, 
            learning_rate_sched=learning_rate_sched, time_decay=time_decay, 
            maxiter=maxiter, precision=precision, i_s=i_s)
filename = "Batch Gradient Descent Time Decay Learning Rate Solution Results.png"            
bgd.plot(directory, filename, show=False)            
```

![](../report/figures/BGD/Lab/Batch Gradient Descent Time Decay Learning Rate Solution Results.png)
`r kfigr::figr(label = "bgd_time_decay_summary_results", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Time Decay Learning Rate Optimal Solution Results 

Ok, let's wrap up this section.

##### Time Decay Learning Rate Key Takeways
What can we take away from this experiment? Two primary points.

1. Since we are reducing the learning rate with each iteration, we set the initial learning rate $\alpha_0$ to the maximum cost reducing learning rate. This value may be obtained via a learning rate test, as shown in `r kfigr::figr(label = "bgd_constant_alpha_plots_i", prefix = TRUE, link = TRUE, type="Figure")`.    
2. Best objective function performance tends to favor lower time decay values. Slowly decaying learning rates tend to yield better, more fine-grained gradient approximations with each iteration.    

Next, we'll explore the step decay learning rates.