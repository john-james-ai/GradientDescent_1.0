<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return ''+n}
}
}});</script>

#### Time Decay Learning Rates
Our purpose here is to identify the time decay learning rate hyperparameters that minimize our objective function on our training set. Recall that, time decay based learning rates adjust according to the number of iterations. Our time decay implementation is mathematically defined as:     
$$\alpha=\alpha_0/(1+kt)$$
where:     

* $\alpha$ is the new learning rate,   
* $\alpha_0$ is the initial learning rate,      
* $t$ is the iteration number,    
* $k$ is the decay hyperparameter.

##### Time Decay Learning Rate Gridsearch 
For our initial alpha, we will use $\alpha_0=1.6$, the maximum learning rate from the learning rate test in `r kfigr::figr(label = "bgd_constant_alpha_plots_i", prefix = TRUE, link = TRUE, type="Figure")`. Using a gridsearch, we will evaluate a range of values for $k$. Again, we'll use default values for our initial $\theta$'s, maxiter, and precision $\epsilon$ parameters. Learning from the last section, we will set the $i_{stop}$ parameter to 20 to grant the algorithm the time to arrive at a suitable optimum. 

```{python bgd_time_params_i, echo=T, eval=T}
theta = np.array([-1,-1]) 
learning_rate_sched = 't'
learning_rate = [1.6]
time_decay = np.arange(0,1,0.01)
precision = [0.001]
maxiter = [5000]
no_improvement_stop=[20]
```

Ok, let's run the search and evaluate performance. 

```{python bgd_time_gs, echo=T, eval=T}
lab = BGDLab()
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, learning_rate=learning_rate, 
               learning_rate_sched=learning_rate_sched, time_decay=time_decay, 
               step_decay=None, step_epochs=None, exp_decay=None,
               precision=precision, maxiter=maxiter, no_improvement_stop=no_improvement_stop)
```

```{python bgd_time_plots, echo=F, eval=T}
filename = 'Batch Gradient Descent - Time Decay - Costs by Learning Rate.png'
lab.figure(data=lab.summary(), x='time_decay', y='final_costs', 
           func=lab.lineplot, directory=directory, 
           filename=filename, width=0.5)             
filename = 'Batch Gradient Descent - Time Decay - Time by Learning Rate.png'
lab.figure(data=lab.summary(), x='time_decay', y='duration', 
           func=lab.lineplot, directory=directory, 
           filename=filename, width=0.5)                        
filename = 'Batch Gradient Descent - Time Decay Report.csv'             
lab.report(directory=directory, filename=filename)
```

![](../report/figures/BGD/Lab/Batch Gradient Descent - Time Decay - Costs by Learning Rate.png){width=50%}![](../report/figures/BGD/Lab/Batch Gradient Descent - Time Decay - Time by Learning Rate.png){width=50%}

`r kfigr::figr(label = "bgd_time_plots", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Training Costs and Computation Time by Time Decay Parameter

```{r bgd_time_report, echo=F, eval=T}
report <- read.csv(file.path(py$directory, py$filename))
report_costs <- report %>% arrange(final_costs, duration) %>% select(learning_rate, time_decay, final_costs, duration)
report_error <- report %>% arrange(final_mse, duration) %>% select(learning_rate, time_decay, final_mse)
```

`r kfigr::figr(label = "bgd_time_plots", prefix = TRUE, link = TRUE, type="Figure")`, shows the relationship between training set costs, computation time and our time decay parameter, $k$. Our highest costs occur at $k=0$, then drops rapidly to a minimum cost of `r round(report_costs['final_costs'][1,],4)` at a time decay parameter $k=$ `r report_costs['time_decay'][1,]`. Similarly, we observe maximum computation time at time decay parameter, $k=0$, followed by a significant reduction. After $k\approx0.1$, computation times appear to stabalize.

##### Time Decay Learning Rate Solution Path
The following animation reveals the effect of a time decayed learning rate over 27 iterations, starting with an initial learning rate $\alpha=1.6$. 

```{python bgd_time_path, echo=F, eval=T, cache=T}
theta = np.array([-1,-1]) 
learning_rate_sched = 't'
learning_rate = 1.6
time_decay = 0.01
precision = 0.001
maxiter = 5000
no_improvement_stop=20
time_decay_demo = BGDDemo()
time_decay_demo.fit(X=X,y=y, theta=theta, learning_rate=learning_rate, 
         learning_rate_sched=learning_rate_sched, time_decay=time_decay,
         precision=precision, maxiter=maxiter, 
         no_improvement_stop=no_improvement_stop)
         
filename = 'Batch Gradient Descent - Time Decay Convergence.gif'
time_decay_demo.show_search(directory=directory, filename=filename, fps=1)
filename = 'Batch Gradient Descent - Time Decay Fit.gif'
time_decay_demo.show_fit(directory=directory, filename=filename, fps=1)
```
![](../report/figures/BGD/Lab/Batch Gradient Descent - Time Decay Convergence.gif)

`r kfigr::figr(label = "bgd_time_path", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Time Decay Convergence k=0.01

The oscillation in the solution path, caused by the very high initial learning rate $\alpha=1.6$, diminishes systematically as the learning rate decays. Optimal cost of $J\approx0.019$ is reached in 27 iterations.  `r kfigr::figr(label = "bgd_time_alpha", prefix = TRUE, link = TRUE, type="Figure")` reveals the learning rates applied at each iteration. 

```{python bgd_time_alpha, echo=F, eval=T}
filename = 'Batch Gradient Descent - Time Decay - Learning Rates by Iteration.png'
data = lab.detail()
data = data[data.time_decay==0.01]
lab.figure(data=data, x='iterations', y='learning_rates', 
           func=lab.lineplot, directory=directory, 
           filename=filename, width=0.5)
last = data['learning_rates'].iloc[-1].item()
```
![](../report/figures/BGD/Lab/Batch Gradient Descent - Time Decay - Learning Rates by Iteration.png)

`r kfigr::figr(label = "bgd_time_alpha", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent - Time Decay Learning Rates by Iteration

##### Time Decay Learning Rate Evaluation
Here, we evaluate the empirical solution obtained from the training set as well as the generalizability of the solution to new data. We will use regression line fit to assess our empirical result. Validation set performance will be our measure of generalizability.   


![](../report/figures/BGD/Lab/Batch Gradient Descent - Time Decay Fit.gif)

`r kfigr::figr(label = "bgd_time_fit", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Time Decay Fit k=0.01

The time decay effect is evident in the regression plot above As a consequence of the high initial learning rate $\alpha_0=1.6$, the oscillation around the optimum is rather dramatic at first. The oscillations diminish in magnitude as the learning rate decays and the norm of the gradient goes to zero. Optimal parameters $\theta$ are obtained in 27 iterations at a minimum cost of $J\approx0.019$

Does our solution generalize well to unseen data? If it did, we would expect that our best performing parameters on the training set would be among the best performing parameters on the validation set. The following plot contrasts training and validation set performance across our parameter sets.

```{python bgd_time_eval, echo=F, eval=T}
filename = 'Batch Gradient Descent Time Decay Learning Rate Costs.png'
lab.figure(data=lab.summary(), x='time_decay', y='final_costs',
           func=lab.lineplot, directory=directory, 
           filename=filename, width=0.5)  
filename = 'Batch Gradient Descent Time Decay Learning Rate Error.png'
lab.figure(data=lab.summary(), x='time_decay', y='final_mse',
           func=lab.lineplot, directory=directory, 
           filename=filename, width=0.5)         
```
![](../report/figures/BGD/Lab/Batch Gradient Descent Time Decay Learning Rate Costs.png){width=50%}![](../report/figures/BGD/Lab/Batch Gradient Descent Time Decay Learning Rate Error.png){width=50%}

`r kfigr::figr(label = "bgd_time_alpha", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Time Decay Learning Rate Costs and Error

The plots look remarkably similar. In fact, both the minimum training set cost $J=$ `r round(report_costs['final_costs'][1,], 4)` and the minimum validation set error, `r round(report_error['final_mse'][1,], 4)`, occur at a time decay value $k=$ `r report_error['time_decay'][1,]`

That said, we can conclude that the solution generalizes reasonably well to unseen data.

Next, we'll explore the step decay learning rate.