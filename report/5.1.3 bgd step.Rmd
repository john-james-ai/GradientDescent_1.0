<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return ''+n}
}
}});</script>

#### Step Decay Learning Rates
Our goal is to identify the step decay hyperparameters that minimize our training set objective function and yields a model that generalizes well to new data. Recall that, step decay based learning rates drop by a factor every few epochs. The mathematical form of step decay is:     
$$\alpha = \alpha_0 * d^{floor((1+t)/t_d)}$$
where:   

* $\alpha$ is the new learning rate,   
* $\alpha_0$ is the initial learning rate,      
* $d$ is the factor by which the learning rate decays each step,    
* $t$ is the iteration or epoch number,    
* $t_d$ is the number of epochs in each step, typically $t_d\approx10$

##### Step Decay Learning Rate Gridsearch 
For our initial alpha, we will use $\alpha_0=1.6$, the maximum learning rate from the learning rate test in `r kfigr::figr(label = "bgd_constant_alpha_plots_i", prefix = TRUE, link = TRUE, type="Figure")`. Using a gridsearch, we will evaluate a range of values for our drop factor, $d$, and the number of epochs per step, $t_d$. Parameters $\theta$, maxiter, precision, and $i_s$ will be set to their default values.

```{python bgd_step_params, echo=T, eval=T}
theta = np.array([-1,-1]) 
learning_rate_sched = 's'
learning_rate = [1.6]
step_decay = np.arange(0,1,0.01)
step_epochs = np.arange(5,25,5)
precision = [0.001]
maxiter = [5000]
i_s=[10]
```

In case you are just joining us, the algorithm stops if maxiter iterations is reached, or the objective function has not improved over $i_s$ iterations. Improvement is measured with a tolerance given by our parmeter, $\epsilon$.  

Ok, let's run the search and evaluate performance. 

```{python bgd_step_filenames, echo=F, eval=T}
bgd_step_decay_summary_filename = 'Batch Gradient Descent Step Decay Learning Rate Summary.csv'
bgd_step_decay_detail_filename = 'Batch Gradient Descent Step Decay Learning Rate Detail.csv'
bgd_step_decay_report_filename = 'Batch Gradient Descent Step Decay Learning Rate Report.csv'
```

```{python bgd_step_gs_echo, echo=T, eval=F}
lab = BGDLab()
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, learning_rate=learning_rate, 
               learning_rate_sched=learning_rate_sched, time_decay=None, 
               step_decay=step_decay, step_epochs=step_epochs, exp_decay=None,
               precision=precision, maxiter=maxiter, i_s=i_s)
```

```{python bgd_step_gs, echo=F, eval=T, cache=F}
lab = BGDLab() 
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, learning_rate=learning_rate, 
               learning_rate_sched=learning_rate_sched, time_decay=None, 
               step_decay=step_decay, step_epochs=step_epochs, exp_decay=None,
               precision=precision, maxiter=maxiter, i_s=i_s)
bgd_step_decay_summary = lab.summary(directory=directory, filename=bgd_step_decay_summary_filename)
bgd_step_decay_detail = lab.detail(directory=directory, filename=bgd_step_decay_detail_filename)
bgd_step_decay_report = lab.report(directory=directory, filename=bgd_step_decay_report_filename)
```

```{r bgd_step_gs_results_r, echo=F, eval=T}
bgd_step_decay_summary <- read.csv(file.path(py$directory, py$bgd_step_decay_summary_filename))
bgd_step_decay_detail <- read.csv(file.path(py$directory, py$bgd_step_decay_detail_filename))
bgd_step_decay_report <- read.csv(file.path(py$directory, py$bgd_step_decay_report_filename))
```

```{python bgd_step_gs_results_python, echo=F, eval=T}
bgd_step_decay_summary = pd.read_csv(os.path.join(directory, bgd_step_decay_summary_filename))
bgd_step_decay_detail  = pd.read_csv(os.path.join(directory, bgd_step_decay_detail_filename))
bgd_step_decay_report  = pd.read_csv(os.path.join(directory, bgd_step_decay_report_filename))

```

##### Step Decay Learning Rate Analysis
```{python bgd_step_plots, echo=F, eval=T}
viz = GradientVisual()
filename = 'Batch Gradient Descent - Step Decay - Costs.png'
viz.figure(alg=alg, data=bgd_step_decay_summary, x='step_decay', y='final_costs', z='step_epochs', 
           func=viz.lineplot, directory=directory, 
           filename=filename, width=0.5)             
filename = 'Batch Gradient Descent - Step Decay - Time.png'
viz.figure(alg=alg, data=bgd_step_decay_summary, x='step_decay', y='duration', z='step_epochs',
           func=viz.lineplot, directory=directory, 
           filename=filename, width=0.5)                        
```

```{python bgd_step_plots_2, echo=F, eval=T}
data = bgd_step_decay_summary
data = data.loc[(data['step_decay'] > 0.2) & (data['step_decay'] < 0.9)]
filename = 'Batch Gradient Descent - Step Decay - Costs II.png'
viz.figure(alg=alg, data=data, x='step_decay', y='final_costs', z='step_epochs', 
           func=viz.lineplot, directory=directory, 
           filename=filename, width=0.5)             
filename = 'Batch Gradient Descent - Step Decay - Time II.png'
viz.figure(alg=alg, data=data, x='step_decay', y='duration', z='step_epochs',
           func=viz.lineplot, directory=directory, 
           filename=filename, width=0.5)                        
          
```

![](../report/figures/BGD/Lab/Batch Gradient Descent - Step Decay - Costs.png){width=50%}![](../report/figures/BGD/Lab/Batch Gradient Descent - Step Decay - Time.png){width=50%}
![](../report/figures/BGD/Lab/Batch Gradient Descent - Step Decay - Costs II.png){width=50%}![](../report/figures/BGD/Lab/Batch Gradient Descent - Step Decay - Time II.png){width=50%}

`r kfigr::figr(label = "bgd_step_plots", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Training Costs and Computation Time by Step Decay Parameters

```{r bgd_step_report, echo=F, eval=T}
bgd_step_decay_report_costs <- bgd_step_decay_report %>% arrange(final_costs, duration) %>% select(learning_rate, step_decay, step_epochs, precision, i_s,  duration, iterations, final_costs)
bgd_step_decay_report_error <- bgd_step_decay_report %>% arrange(final_mse, duration) %>% select(learning_rate, step_decay, step_epochs, precision, i_s,  duration, iterations, final_mse)
```

The four plots in `r kfigr::figr(label = "bgd_step_plots", prefix = TRUE, link = TRUE, type="Figure")`, show the relationship between training set costs, computation time and our step decay parameters. The first row represents the full range of parameter values. We've narrowed the range of step decay values on the second row so that we exclude outliers.

Lower training set costs appear to correlate with greater numbers of epochs per step and higher decay rates. Computation times are clearly associated with the number of epochs per step. Step decay rates don't appear to be a computation time factor.

Which step decay hyperparameters produced the best objective function performance?

`r kfigr::figr(label = "bgd_best_step", prefix = TRUE, link = TRUE, type="Table")`: Batch Gradient Descent Best Performing Step Decay Parameters
```{r bgd_best_step, echo=F, eval=T}
knitr::kable(head(bgd_step_decay_report_costs,3), format.args = list(big.mark = ",")) %>%
 kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = F, position = "center")
```

The above table summarizes the three best step decay parameter settings. We'll analyze and evaluate step decay value $d=$ `r bgd_step_decay_report_costs['step_decay'][1,]` and epochs per step $t_d=$ `r bgd_step_decay_report_costs['step_epochs'][1,]`.

##### Step Decay Learning Rate Solution Path
The following animation reveals the effect of our step decay learning rate over `r bgd_step_decay_report_costs['iterations'][1,]` iterations, starting with an initial learning rate $\alpha=$ `r bgd_step_decay_report_costs['learning_rate'][1,]`. 

```{python bgd_step_final_params, echo=F, eval=T}
theta = np.array([-1,-1]) 
learning_rate_sched = 's'
learning_rate = bgd_step_decay_report['learning_rate'].iloc[0]
step_decay = bgd_step_decay_report['step_decay'].iloc[0]
step_epochs = bgd_step_decay_report['step_epochs'].iloc[0]
precision = bgd_step_decay_report['precision'].iloc[0]
maxiter = bgd_step_decay_report['maxiter'].iloc[0]
i_s = bgd_step_decay_report['i_s'].iloc[0]
```

```{python bgd_step_decay_demo_filenames, echo=F, eval=T}
bgd_step_decay_path_demo_filename = 'Batch Gradient Descent - Step Decay Convergence.gif'
bgd_step_decay_fit_demo_filename = 'Batch Gradient Descent - Step Decay Fit.gif'
```


```{python bgd_step_path, echo=F, eval=T, cache=F}
step_decay_demo = BGDDemo() 
step_decay_demo.fit(X=X,y=y, theta=theta, learning_rate=learning_rate, 
         learning_rate_sched=learning_rate_sched, step_decay=step_decay,
         step_epochs=step_epochs, precision=precision, maxiter=maxiter, 
         i_s=i_s)
         
step_decay_demo.show_search(directory=directory, filename=bgd_step_decay_path_demo_filename, fps=1)
step_decay_demo.show_fit(directory=directory, filename=bgd_step_decay_fit_demo_filename, fps=1)
```
![](../report/figures/BGD/Lab/Batch Gradient Descent - Step Decay Convergence.gif)

`r kfigr::figr(label = "bgd_step_path", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Step Decay Learning Rate Solution Path

The effects of the high initial learning rate and the large epochs per step parameter are visibly extant in `r kfigr::figr(label = "bgd_step_path", prefix = TRUE, link = TRUE, type="Figure")`. For the first 20 epochs, the solution dramatically oscillates around an optimum with seemingly little improvement. At epoch 21, the learning rate is decayed by a factor of 0.88, which puts the optimization on the path to convergence. The gradient is reduced at a faster rate, convergence accelerates, and the algorithm settles at a cost $J\approx$ `r round(bgd_step_decay_report_costs['final_costs'][1,],3)` in `r bgd_step_decay_report_costs['iterations'][1,]` epochs. 

##### Step Decay Learning Rate Evaluation
In this section, we ask two questions.  Is the solution that indeed optimal, and does it generalize well? To the former, we examine the solution's fit to the test data. For the latter question, we evaluate the validation set error of the solution.

`r kfigr::figr(label = "bgd_step_fit", prefix = TRUE, link = TRUE, type="Figure")` shows a scatter plot of the training data. The regression lines given by the $\theta$'s computed for each epoch, are juxtaposed. A good regression line fit suggests a high quality approximation of the optimal solution. 
![](../report/figures/BGD/Lab/Batch Gradient Descent - Step Decay Fit.gif)

`r kfigr::figr(label = "bgd_step_fit", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Step Decay Learning Rate Fit

As expected, the algorithm oscillates dramatically around the optimal fit, making little progress until epoch `r bgd_step_decay_report_costs['step_epochs'][1,]`. At this stage, the magnitude of the oscillation diminishes fairly rapidly, and the algorithm settles into its optimum. The quality of the fit suggests a good approximation of the true parameters $\theta$.

How does the solution generalize to new data? If the optimal training set parameters performed similarly well on the validation set, we can infer that the solution will generalize well to unseen data.   

```{python bgd_step_eval, echo=F, eval=T}
data = bgd_step_decay_summary
data = data.loc[(data['step_decay'] > 0.5) & (data['step_decay'] < 0.9) & (data['step_epochs']==20)]
filename = 'Batch Gradient Descent - Step Decay Eval - Costs.png'
viz.figure(alg=alg, data=data, x='step_decay', y='final_costs',  
           func=viz.lineplot, directory=directory, 
           filename=filename, width=0.5)             
filename = 'Batch Gradient Descent - Step Decay Eval - Error.png'
viz.figure(alg=alg, data=data, x='step_decay', y='final_mse', 
           func=viz.lineplot, directory=directory, 
           filename=filename, width=0.5)                        
```
![](../report/figures/BGD/Lab/Batch Gradient Descent - Step Decay Eval - Costs.png){width=50%}![](../report/figures/BGD/Lab/Batch Gradient Descent - Step Decay Eval - Error.png){width=50%}
`r kfigr::figr(label = "bgd_step_eval", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Step Decay Learning Rate Cost and Error Evaluation

```{r bgd_step_eval_ii_data, echo=F, eval=T}
bgd_step_decay_eval_train <- bgd_step_decay_report %>% arrange(final_costs, duration) %>% select(learning_rate, step_decay, step_epochs, precision, i_s,  duration, iterations, final_costs, final_mse)
bgd_step_decay_eval_val <- bgd_step_decay_report %>% arrange(final_mse, duration) %>% select(learning_rate, step_decay, step_epochs, precision, i_s,  duration, iterations, final_costs, final_mse)
```

The above two plots show training set costs (left) and validation set error (right) for our step decay learning rate parameters. Both plots have very similar shapes and appear to minimize at or near the same step decay value. 

`r kfigr::figr(label = "bgd_step_eval_ii_tables", prefix = TRUE, link = TRUE, type="Table")`: Batch Gradient Descent Step Decay Learning Rate Best Empirical and Expected Results
```{r bgd_step_eval_ii_tables, echo=F, eval=T}
knitr::kable(head(bgd_step_decay_eval_train,3), format.args = list(big.mark = ",")) %>%
 kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = F, position = "center")
knitr::kable(head(bgd_step_decay_eval_val,3), format.args = list(big.mark = ",")) %>%
 kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = F, position = "center")
```

The top table in `r kfigr::figr(label = "bgd_step_eval_ii_tables", prefix = TRUE, link = TRUE, type="Table")` shows the top 3 performing hyperparameter sets obtained from training. The bottom table lists the same, but by validation set error. Both hyperparameter sets minimize the validation set error to `r round(bgd_step_decay_eval_train['final_mse'][1,],4)`. Therefore, it would be sensible to infer that our solution generalizes well to unseen data. 

##### Step Decay Learning Rate Solution Summary
To summarize, we obtained an optimal solution of $J\approx$ `r round(bgd_step_decay_report_costs['final_costs'][1,], 4)`, within about `r round(bgd_step_decay_report_costs['duration'][1,],3)` milliseconds with the following parameters:

`r kfigr::figr(label = "bgd_step_decay_summary_params", prefix = TRUE, link = TRUE, type="Table")`: Batch Gradient Descent Step Decay Learning Rate Solution Hyperparameters
```{r bgd_step_decay_summary_params}
params <- bgd_step_decay_report_costs %>% select(learning_rate, step_decay, step_epochs, precision, maxiter, i_s) 
knitr::kable(t(params[1,]), format.args = list(big.mark = ",")) %>%
 kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = F, position = "center")
```

The optimal results, in terms of training set costs and validation set error are:
```{python bgd_step_decay_summary_results, echo=F, eval=T}
bgd = BGD()
bgd.fit(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, learning_rate=learning_rate, 
            learning_rate_sched=learning_rate_sched, step_decay=step_decay, 
            step_epochs=step_epochs, maxiter=maxiter, precision=precision, i_s=i_s)
filename = "Batch Gradient Descent Step Decay Learning Rate Solution Results.png"            
bgd.plot(directory, filename, show=False)            
```

![](../report/figures/BGD/Lab/Batch Gradient Descent Step Decay Learning Rate Solution Results.png)
`r kfigr::figr(label = "bgd_step_decay_summary_results", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Step Decay Learning Rate Optimal Solution Results 

Ok, let's wrap up this section.

##### Step Decay Learning Rate Key Takeaways
What have we observed from this experiment? Well, the number of epochs in each step tended to have a greater effect on objective function performance, than did the step decay factor. Best performance correlated with higher numbers of epochs per step. One just needs to ensure that the number of epochs per step doesn't exceed the early stopping convergence criteria.

Properly setting the step decay hyperparameters requires experimentation as the choice of decay rates and epochs per step is largely a function of the size of the dataset, the contours of the objective function and available computational resources and time.

Next, we explore the effects of exponential decay learning rates. 