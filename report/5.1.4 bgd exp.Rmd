<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return ''+n}
}
}});</script>
#### Exponential Decay Learning Rates
Exponential decay learning rates have the following mathematical formulation:    
$$\alpha = \alpha_0 \times e^{(-kt)}$$  
where:    

* $\alpha$ is the new learning rate,   
* $\alpha_0$ is the initial learning rate,      
* $t$ is the iteration number,    
* $k$ is a hyperparameter.

Since we are decaying the learning rate each iteration, $\alpha_0$ is typically set high. Our learning rate test (`r kfigr::figr(label = "bgd_constant_alpha_plots_i", prefix = TRUE, link = TRUE, type="Figure")`) revealed 1.6 to be the highest learning rate that still reduced costs on our training set. This will serve as our $\alpha_0$ parameter.  Our goal; therefore, is to find a parameter $k$ that minimizes our training set objective function and yields a model that generalizes well to new data. 

##### Exponential Decay Learning Rate Gridsearch 
Using a gridsearch, we will evaluate a range of values for our exponential decay factor, $k$. Parameters $\theta$, maxiter, precision, and $i_s$ will be set to their default values.

```{python bgd_exp_params, echo=T, eval=T}
theta = np.array([-1,-1]) 
learning_rate_sched = 'e'
learning_rate = [1.6]
exp_decay = np.arange(0.01,1,0.01)
precision = [0.001]
maxiter = 5000
i_s=[10]
```

In case you are just joining us, the algorithm stops if maxiter iterations is reached, or the objective function has not improved over $i_s$ iterations. Improvement is measured with a tolerance given by our precision parameter, $\epsilon$.  

Ok, let's run the search and evaluate performance. 

```{python bgd_exp_filenames, echo=F, eval=T}
bgd_exp_decay_summary_filename = 'Batch Gradient Descent Exponential Decay Learning Rate Summary.csv'
bgd_exp_decay_detail_filename = 'Batch Gradient Descent Exponential Decay Learning Rate Detail.csv'
bgd_exp_decay_report_filename = 'Batch Gradient Descent Exponential Decay Learning Rate Report.csv'
```


```{python bgd_exp_gs_echo, echo=T, eval=F}
lab = BGDLab()
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, learning_rate=learning_rate, 
               learning_rate_sched=learning_rate_sched, time_decay=None, 
               step_decay=None, step_epochs=None, exp_decay=exp_decay,
               precision=precision, maxiter=maxiter, i_s=i_s)
```


```{python bgd_exp_gs, echo=F, eval=T}
lab = BGDLab()
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, learning_rate=learning_rate, 
               learning_rate_sched=learning_rate_sched, time_decay=None, 
               step_decay=None, step_epochs=None, exp_decay=exp_decay,
               precision=precision, maxiter=maxiter, i_s=i_s)
bgd_exp_decay_summary = lab.summary(directory=directory, filename=bgd_exp_decay_summary_filename)
bgd_exp_decay_detail = lab.detail(directory=directory, filename=bgd_exp_decay_detail_filename)
bgd_exp_decay_report = lab.report(directory=directory, filename=bgd_exp_decay_report_filename)               
```

```{r bgd_exp_gs_results_r, echo=F, eval=T}
bgd_exp_decay_summary <- read.csv(file.path(py$directory, py$bgd_exp_decay_summary_filename))
bgd_exp_decay_detail <- read.csv(file.path(py$directory, py$bgd_exp_decay_detail_filename))
bgd_exp_decay_report <- read.csv(file.path(py$directory, py$bgd_exp_decay_report_filename))
```


##### Exponential Decay Learning Rate Analysis 
```{python bgd_exp_plots, echo=F, eval=T}
viz = GradientVisual()
filename = 'Batch Gradient Descent - Exponential Decay Learning Rate - Costs.png'
viz.figure(alg=alg, data=bgd_exp_decay_summary, x='exp_decay', y='final_costs',
           func=viz.lineplot, directory=directory, 
           filename=filename, width=0.5)             
filename = 'Batch Gradient Descent - Exponential Decay Learning Rate - Time.png'
viz.figure(alg=alg, data=bgd_exp_decay_summary, x='exp_decay', y='duration',
           func=viz.lineplot, directory=directory, 
           filename=filename, width=0.5)
```

The following plots reveal the relationship between training set costs, computation time, and the exponential decay factor.

![](../report/figures/BGD/Lab/Batch Gradient Descent - Exponential Decay Learning Rate - Costs.png){width=50%}![](../report/figures/BGD/Lab/Batch Gradient Descent - Exponential Decay Learning Rate - Time.png){width=50%}

`r kfigr::figr(label = "bgd_exp_plots", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Training Costs and Computation Time by Exponential Decay Rate

Based upon `r kfigr::figr(label = "bgd_exp_plots", prefix = TRUE, link = TRUE, type="Figure")` (left) objective function costs correlated with the exponential decaying factor. In other words, objective function performance declined with increasing exponential decay factors. 

As indicated in `r kfigr::figr(label = "bgd_exp_plots", prefix = TRUE, link = TRUE, type="Figure")` (left), objective function performance comes at  a cost. The best performing exponential decay factor had a significantly greater computation time.

So, which exponential decay value yielded best objective function performance? 

```{r bgd_exp_report, echo=F, eval=T}
bgd_exp_decay_report_costs <- bgd_exp_decay_report %>% arrange(final_costs, duration) %>% select(learning_rate, exp_decay, final_costs, duration)
bgd_exp_decay_report_error <- bgd_exp_decay_report %>% arrange(final_mse, duration) %>% select(learning_rate, exp_decay, final_mse)
rel_chg_cost <- round(((1 - (bgd_exp_decay_report_costs['final_costs'][1,] / bgd_exp_decay_report_costs['final_costs'][2,])) * 100),2)
rel_chg_time <- round(((1 - (bgd_exp_decay_report_costs['duration'][2,] / bgd_exp_decay_report_costs['duration'][1,] )) * 100),2)
```

`r kfigr::figr(label = "bgd_best_exp", prefix = TRUE, link = TRUE, type="Table")`: Batch Gradient Descent Best Performing Exponential Decay Rates
```{r bgd_best_exp, echo=F, eval=T}
knitr::kable(head(bgd_exp_decay_report_costs,3), format.args = list(big.mark = ",")) %>%
 kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = F, position = "center")
```

As indicated in `r kfigr::figr(label = "bgd_best_exp", prefix = TRUE, link = TRUE, type="Table")`, the best performing exponential decay factor was $k=$ `r bgd_exp_decay_report_costs['exp_decay'][1,]`, yielding a training set cost of $J\approx$ `r round(bgd_exp_decay_report_costs['final_costs'][1,],4)` in about `r round(bgd_exp_decay_report_costs['duration'][1,],4)` milliseconds. It is also worth noting that the a `r rel_chg_cost` percent sacrifice in objective function performance reduces our computation time by `r rel_chg_time` percent.

Let's see what the solution path reveals about algorithm behavior vis-a-vis our best performing decaying factor $k$. 

##### Exponential Decay Learning Rate Solution Path
The optimization path depicted in `r kfigr::figr(label = "bgd_exp_path", prefix = TRUE, link = TRUE, type="Figure")` is not unlike the time based decay learning rate solution path from `r kfigr::figr(label = "bgd_time_path", prefix = TRUE, link = TRUE, type="Figure")`.

```{python bgd_exp_decay_learning_rate_path, echo=F, eval=T, cache=F}
bgd_exp_decay_best = bgd_exp_decay_report.nsmallest(1, ['final_costs', 'duration'])
experiment = bgd_exp_decay_best['experiment'].iloc[0]
bgd_exp_decay_best_summary = bgd_exp_decay_summary.loc[bgd_exp_decay_summary['experiment'] == experiment]
bgd_exp_decay_best_detail = bgd_exp_decay_detail.loc[bgd_exp_decay_detail['experiment'] == experiment]

bgd_exp_decay_search_filename = 'Batch Gradient Descent - Exp Decay Convergence.gif'
bgd_exp_decay_fit_filename = 'Batch Gradient Descent - Exp Decay Fit.gif'

bgd = BGD()
X_scaled, y_scaled = bgd.prep_data(X,y)
viz = GradientVisual()
viz.show_search(alg, X_scaled, y_scaled, bgd_exp_decay_best_detail, bgd_exp_decay_best_summary, 
               directory=directory, filename=bgd_exp_decay_search_filename, fps=1) 
viz.show_fit(alg, X_scaled, y_scaled, bgd_exp_decay_best_detail, bgd_exp_decay_best_summary, 
               directory=directory, filename=bgd_exp_decay_fit_filename, fps=1)   
```
![](../report/figures/BGD/Lab/Batch Gradient Descent - Exp Decay Convergence.gif)

`r kfigr::figr(label = "bgd_exp_path", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Exponential Decay Learning Rate Solution Path

We observe the same large oscillation about the optimum that we observed for the time decay learning rate solution. The decaying factor systematically diminishes the magnitude of the oscillation, resulting in a gradient that converges to its minimum cost of $J\approx$ `r round(bgd_exp_decay_report['final_costs'][1,],3)`in `r bgd_exp_decay_report['iterations'][1,]` epochs. 

Now, let's evaluate the empirical and expected performance of our solution.

##### Exponential Decay Learning Rate Evaluation
Here, we ask two questions. What is the empirical quality of our solution and how well does the solution generalize to unseen data? We assess the quality of the empirical performance on the training set by examining how well the solution fits the training data.  Validation set error will be the metric by which we measure expected performance on unseen data. 

With repect to empirical performance, `r kfigr::figr(label = "bgd_exp_fit", prefix = TRUE, link = TRUE, type="Figure")` shows a scatter plot of the training data. Juxtaposed upon this, the regression lines given by the $\theta$'s computed each epoch. A good regression line fit indicates a high quality empirical result. 

![](../report/figures/BGD/Lab/Batch Gradient Descent - Exp Decay Fit.gif)

`r kfigr::figr(label = "bgd_exp_fit", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Exponential Decay Learning Rate Fit

The early regression lines evidence the large oscillations observed in `r kfigr::figr(label = "bgd_exp_path", prefix = TRUE, link = TRUE, type="Figure")`; however, the energy caused by the large initial learning rate is slowly released by the exponential decaying factor. The solution ultimately converges to a reasonably good fit to the data. We can therfore infer that the empirical solution is a good approximation of the true model.

Does the solution generalize? If it did, we would expect our empirical solution to perform well on the validation set.  

```{python bgd_exp_eval, echo=F, eval=T}
filename = 'Batch Gradient Descent - Exp Decay Eval - Costs.png'
viz.figure(alg=alg, data=bgd_exp_decay_summary, x='exp_decay', y='final_costs',  
           func=viz.lineplot, directory=directory, 
           filename=filename, width=0.5)             
filename = 'Batch Gradient Descent - Exp Decay Eval - Error.png'
viz.figure(alg=alg, data=bgd_exp_decay_summary, x='exp_decay', y='final_mse', 
           func=viz.lineplot, directory=directory, 
           filename=filename, width=0.5)                        
```
![](../report/figures/BGD/Lab/Batch Gradient Descent - Exp Decay Eval - Costs.png){width=50%}![](../report/figures/BGD/Lab/Batch Gradient Descent - Exp Decay Eval - Error.png){width=50%}
`r kfigr::figr(label = "bgd_exp_eval", prefix = TRUE, link = TRUE, type="Figure")`: Batch Gradient Descent Exponential Decay Learning Rate Cost and Error Evaluation

Above, we have our exponential decay factors plotted vis-a-vis training set costs (left) and validation set error (right). The curves are nearly identical. In fact, both curves are minimized at $k=0.01$. The validation set performance of our solution strongly suggests the generalizability of our solution.   

##### Exponential Decay Learning Rate Solution Summary
To summarize, we obtained an optimal solution of $J\approx$ `r round(bgd_exp_decay_report_costs['final_costs'][1,], 4)`, within about `r round(bgd_exp_decay_report_costs['duration'][1,],3)` milliseconds with the following parameters:

`r kfigr::figr(label = "bgd_exp_decay_summary_params", prefix = TRUE, link = TRUE, type="Table")`: Batch Gradient Descent Exponential Decay Learning Rate Solution Hyperparameters
```{r bgd_exp_decay_summary_params}
params <- bgd_exp_decay_report %>% select(learning_rate, exp_decay, precision, maxiter, i_s) 
knitr::kable(t(params[1,]), format.args = list(big.mark = ",")) %>%
 kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = F, position = "center")
```


Ok, let's wrap up this section.

##### Exponential Decay Learning Rate Key Takeaways
What have we observed in this experiment?  A few points:  

1. For our dataset, objective function performance was inversely correlated with the exponential decay factor, $k$.  
2. When selecting among several high performing exponential decay factors, it is worth noting their computational effects. As shown in this experiment, we were able to reduce computation time by `r rel_chg_time` percent with just a `r rel_chg_cost` percent reduction in objective function performance. 

