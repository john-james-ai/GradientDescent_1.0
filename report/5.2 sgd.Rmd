<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return ''+n}
}
}});</script>

```{python GradientDescent, code=readLines('../src/GradientDescent.py'), message=FALSE, warning=FALSE}
```
```{python GradientDemo, code=readLines('../src/GradientDemo.py'), message=FALSE, warning=FALSE}
```
```{python GradientFit, code=readLines('../src/GradientFit.py'), message=FALSE, warning=FALSE}
```
```{python GradientLab, code=readLines('../src/GradientLab.py'), message=FALSE, warning=FALSE}
```
```{python GradientVisual, code=readLines('../src/GradientVisual.py'), message=FALSE, warning=FALSE}
```


```{python sgd_defaults, echo=F, eval=T}
directory = "./report/figures/SGD/Lab"
alg = "Stochastic Gradient Descent"
```
## Stochastic Gradient Descent 
Stochastic gradient descent (SGD), in constrast to batch gradient descent, performs a parameter update for *each* training example $x^{(i)}$ and label $y^{(i)}$:

$$\theta_j := \theta_j-\alpha \nabla_\theta J(\theta, x^{(i)};y^{(i)}).$$

> What are the advantages of stochastic gradient descent?

### Advantages of Stochastic Gradient Descent
The primary advantages of stochastic gradient descent are time complexity for large data sets, and convergence in certain non-convex machine learning models.

#### Time Complexity
Since only one data point is used per iteration, it takes only $O(n)$ time. To process an entire epoch of stochastic gradient descent takes approximately the same amount of time as just one iteration of *batch* gradient descent. The question then becomes, which algorithm performs better, $k$ epochs of stochastic gradient descent or $k$ iterations of gradient descent. With extremely large datasets stochastic gradient descent comes out as the clear winner. Consider a dataset with 1 billion data points.  Gradient descent performs a sum over all data points to perform a single update. Stochastic gradient descent makes progress on each training example and converges much faster than gradient descent. When computational resources are the bottleneck, stochastic gradient descent is the dominant paradigm in modern large-scale machine learning.

#### Non-Convex Convergence
Optimizing a non-convex function is challenging in two respects. First, non-convex functions may have many local minima and it may be difficult to find the global minimum among them. Second, even finding a local minimum may be difficult due to the presence of saddle points which have a zero-gradient, but are not local minima. 

Let's be clear. There is no algorithm that guarantees convergence of non-convex objective functions to a global minimum. 

That said, the frequency with which SGD performs its updates results in higher variance and in additional 'noise' in the convergence path.  This randomness enables SGD to avoid some local minimums and escape some saddle points [@Ge2015]. Here's why.

![](../report/figures/Saddle_point.svg.png) 
Saddle Point. Digital Image. Nicoguaro https://commons.wikimedia.org/wiki/File:Saddle_point.svg

`r kfigr::figr(label = "saddle_point", prefix = TRUE, link = TRUE, type="Figure")`
Consider the saddle point in   `r kfigr::figr(label = "saddle_point", prefix = TRUE, link = TRUE, type="Figure")`. The key observation is that the saddle point is actually very unstable. If you were to nudge the red dot slightly in any direction, it is likely to fall off the saddle point. Stochastic gradient descent's inherent noise is the nudge that pushes the red dot off the zero gradient saddle point. 

> What are the challenges of stochastic gradient descent?

### Challenges of Stochastic Gradient Descent

The most challenging aspect of stochastic gradient descent is selection of the learning rate. As with batch gradient descent, a learning rate that is too small will retard convergence.  One that is too large may explode the gradient or prevent the algorithm from converging all together. But the learning rate plays an additional role with SGD. Learning rates tend to suppress or amplify the inherent noise in the gradient. As such, the SGD learning rate has a greater effect on the behavior of the gradient. 

A practitioner may designate a constant learning rate, a learning rate schedule (annealing), or an adaptive learning rate. A constant learning rate, the default in the Keras SGD optimizer, requires experimentation with a range of learning rates in order to achieve suitable performance. Learning rate schedules, such as time-based decay, step decay, and exponential decay, adjust the learning rate according to the number of iterations or epochs. The hyperperameters for the learning rate schedule must be defined a-priori. Another challenge is that the learning rate applies to all parameters during the parameter update. Adaptive learning rate methods such as Adagrad, Adadelta, RMSProp, and Adam are gradient descent optimization algorithms have performed well vis-a-vis learning rate schedule and constant learning rate methods. They will be discussed later in this series. 

Finally, SGD is sequential in nature and doesn't take advantage of vectorized code optimizations. Hence classic SGD can be computationally inefficient.

> When to use stochastic gradient descent?

In short, stochastic gradient descent is a sensible choice for large scale learning problems when computation time is the bottleneck. On large datasets (> 500k observations), SGD is less memory intensive and it often converges faster than batch gradient descent because it performs updates more frequently. Moreover, SGD can achieve better expected risk because more training examples are processed during the available computation time [@Bottou].

SGD is also appropriate for nonlinear networks with multiple local minima and other applications in which the noisy gradient is an advantage for the optimization problem. 

Lastly, SGD is well suited in online settings where the function being modeled is changing over time. This is quite common in industrial settings in which equipment wear and tear can cause the data distribution to change gradually over time [@LeCunBOM12].

In the following sections, we will implement a basic stochastic gradient descent class, perform hyperparameter tuning using a gridsearch, then evaluate algorithm behavior for select hyperparameter sets. 

### Implementation
The basic pseudocode for the SGD algorithm looks like this. 
```{r eval=F, echo=T, tidy=F}
randomly initialize parameters theta, 
set learning rate, maximum interations, and precision
Repeat until approximate convergence
    Randomly shuffle the training set
    For i = 1,2,.., m do:
        costs += compute_costs(data, thetas)
        grad = evaluate_gradient(data, thetas)
        thetas = thetas - learning rate * grad
return(thetas)
```
`r kfigr::figr(label = "sgd_algorithm", prefix = TRUE, link = TRUE, type="Figure")`: Stochastic Gradient Descent Algorithm

Before we get into the weeds, let's align on a few design considerations. 

#### Design Considerations
Our SGD class will inherit the following methods from the GradientDescent abstract base class created in [enter link to BGD]    

* hypothesis - computes the hypothesis for a given observation or a full data set,   
* error - computes the error associated with a hypothesis and $y$.    
* cost - computes the cost given $y$ and our hypothes(es).     
* mse - computes mean squared error for a validation set $X$ and $y$.      
* gradient - calculates the gradient based upon error and the values $X_j$ for each $j$.      
* update - computes the updates to the $\theta$'s in the direction of greatest descent.    

Our SGD class will implement the following two methods:

* shuffle - shuffles the data at the beginning of each epoch, 
* fit - method that implements the algorithm specified in `r kfigr::figr(label = "sgd_algorithm", prefix = TRUE, link = TRUE, type="Figure")`. 

SGD will also inherit support for the following learning rate schedules from the GradientDescent base class:

* constant learning rate,    
* time decay learning rates,    
* step decay learning rates, and    
* exponential decay learning rates.   

With respect to stopping criteria, we adopted an objective function performance based early stopping criteria for batch gradient descent. Since SGD creates noisy gradients that don't necessarily improve objection function performance quite the way that batch methods do, we will expand our stopping criteria to include: 

1. **Validation Set Performance**: A proportion of the training set is designated as a validation set. On each iteration, validation set error is computed and the algorithm stops if validation error has not improved, within a precision $\epsilon$, over $i_s$ epochs.
2. **Gradient Checking**: The norm of the gradient is computed each epoch. The algorithm stops if the magnitude of the gradient has not changed, within a precision $\epsilon$,  over $i_s$ epochs.

Those are the essential considerations.  Let's get coding!

#### GradientDescent Abstract Base Class
Our implementation will be a subclass of our GradientDescent base class below. 
```{python class_init, echo=T, eval=F}
from abc import ABC, abstractmethod
import datetime
import math
import numpy as np
import pandas as pd

class GradientDescent(ABC):
    def __init__(self):
      pass
      
    def _hypothesis(self, X, theta):
        return(X.dot(theta))
        
    def _error(self, h, y):
        return(h-y)
        
    def _cost(self, e):
        return(1/2 * np.mean(e**2))    
        
    def _total_cost(self, X, y, theta):
        h = self._hypothesis(X, theta)
        e = self._error(h, y)
        return(self._cost(e))        
        
    def _mse(self, X, y, theta):
        h = self._hypothesis(X, theta)
        e = self._error(h,y)
        return(np.mean(e**2))
        
    def _gradient(self, X, e):
        return(X.T.dot(e)/X.shape[0])    
        
    def _update(self, alpha, theta, gradient):
        return(theta-(alpha * gradient))        
        
    def _update_learning_rate(self, learning_rate_init, learning_rate_sched, learning_rate, 
                              time_decay, step_decay, step_epochs, exp_decay, epoch):
        if learning_rate_sched == 'c':  # Constant learning rate
            learning_rate_new = learning_rate
        elif learning_rate_sched == 't':  # Time decay learning rate
            learning_rate_new = learning_rate_init/(1+time_decay*epoch)            
        elif learning_rate_sched == 's':  # Step decay learning rate
            learning_rate_new = learning_rate_init*math.pow(step_decay, math.floor((1+epoch)/step_epochs))
        elif learning_rate_sched == 'e':  # Exponential decay learning rate
            learning_rate_new = learning_rate_init * math.exp(-exp_decay*epoch)
        return(learning_rate_new)        
        
    def _finished(self, state, maxiter, precision, i_s):
        if state['iteration'] == maxiter:
            return(True)        
        if abs(state['prior']-state['current']) < precision:
            self._iter_no_change += 1
            if self._iter_no_change >= i_s:
                return(True)
            else:
                return(False)
        else:
            self._iter_no_change = 0   
            return(False)
              
    def _update_state(self,state, cross_validated, iteration, J, mse):
        state['iteration'] = iteration
        state['prior'] = state['current']
        if cross_validated:
            state['current'] = mse
        else:
            state['current'] = J
        return(state)
        
    @abstractmethod
    def fit(self):
        pass
        
```

#### Stochastic Gradient Descent Fit Method
The SGD class will have a concrete implementation of the fit method to implement the algorithm described in `r kfigr::figr(label = "sgd_algorithm", prefix = TRUE, link = TRUE, type="Figure")`. Note that we've added a new parameter, 'stop_metric', which indicates the metric used for early stopping.  Valid values include:    

* 'j': training set costs (default),     
* 'v': validation set error, and    
* 'g': gradient norm. 


```{python sgd_fit, echo=T, eval=F}
def fit(self, X, y, theta, X_val=None, y_val=None, learning_rate=0.01, 
        learning_rate_sched = 'c', time_decay=None, step_decay=None,
        step_epochs=None, exp_decay=None, maxiter=0, precision=0.001, 
        stop_metric='j', i_s=5, scaler='minmax'):

    # Initialize search variables
    iteration = 0
    epoch = 0
    self._iter_no_change = 0
    
    # Set cross validated variable to True if validation set data provided
    cross_validated = all(v is not None for v in [X_val, y_val])
    
    # Initialize State 
    state = {'prior':100, 'current':10, 'iteration':0}
    
    while not self._finished(state):
        epoch += 1            
        mse = None

        X, y = self._shuffle(self._X, self._y)            

        for x_i, y_i in zip(X.values, y):
            iteration += 1

            h = self._hypothesis(x_i, theta)
            e = self._error(h, y_i)
            J = self._cost(e)                         
            g = self._gradient(x_i, e)
            theta = self._update(theta, learning_rate, g)
        
        # Compute total training set cost with latest theta
        J = self._total_cost(X, y, theta)

        # if cross_validated, compute validation set MSE 
        if cross_validated: 
            mse = self._mse(X_val, y_val, theta)
            self._mse_history.append(mse)
            
        # Compute norm of the gradient
        g_norm = np.sqrt(g.dot(g))

        # Update state to include current iteration, loss and mse (if cross_validated)
        state = self._update_state(state, iteration, J, mse, g_norm)

        # Update learning rate
        learning_rate = self._update_learning_rate(learning_rate, epoch)
    return(theta)
```

We will need to change our state update method in the GradientDescent class as follows:
```{python state_update, echo=T, eval=F}
def _update_state(self, state, iteration, J, mse, g):
    state['iteration'] = iteration
    state['prior'] = state['current']
    if self._request['hyper']['stop_metric'] == 'j':
        state['current'] = J
    elif self._request['hyper']['stop_metric'] == 'v':
        state['current'] = mse
    elif self._request['hyper']['stop_metric'] == 'g':
        state['current'] = g            
    return(state)
```


Finally, our shuffle method:
```{python shuffle, echo=T, eval=F}
def _shuffle(self, X, y):
    y_var = y.name
    df = pd.concat([X,y], axis=1)
    df = df.sample(frac=1, replace=False, axis=0)
    X = df.drop(labels = y_var, axis=1)
    y = df[y_var]
    return(X, y)
```


That's it, for the most part. The complete class definitions are available at https://github.com/DecisionScients/GradientDescent.

Next, we'll explore hyperparamter tuning strategies for stochastic gradient descent.

### Tuning Stochastic Gradient Descent
Our hyperparameters for stochastic gradient descent are:  

* **Initial condition hyperparameters**:    
    - initial weight parameters $\theta$     
* **learning rate hyperparameters**:    
    - learning rate $\alpha$, this will be the initial learning rate for learning rate schedules    
    - learning rate schedule, which can be:      
        * 'c' for constant learning rate,    
        * 't' for time decay learning rates,      
        * 's' for step decay learning rates, and      
        * 'e' for exponential decay learning rates
    - other learning rate hyperparameters such as:     
        * time_decay: the parameter that determines the amount the learning rate decays on each iteration,     
        * step_decay: the parameter that determines the amount the learning rate decays on each step,     
        * step_epochs: the parameter that specifies the number of epochs in each step,     
        * exp_decay: the amount of exponential decay to apply to the learning rate on each iteration.     

* **stopping condition hyperparameters**:
    - stop metric: Indicates metric evaluated for early stopping. Valid values are 'j', 'v', and 'g' for training set costs, validation set error, and gradient norm respectively,  
    - precision parameter $\epsilon$ by which we evaluate whether a quantum, e.g. training set costs, has changed,   
    - $i_s$, the algorithm stops when the number of consecutive epochs of no change in the stop metric equals $i_s$.

As with batch gradient descent, learning rate is the most consequential of the hyperparameters. We will use a gridsearch strategy to create candidate functions $f_θ(x)$ for a range of learning rate schedules and learning rates. We will also examine the effects of various early stopping criteria. The initial $\theta$’s will be set to the same  default values used for BGD.

#### The Data
We will use the same data used for the batch gradient descent section.  In case you are just joining us, our training and validation sets were randomly selected from our Ames Housing Project Training Set.  The training set is comprised of random sample of 500 observations. The remaining 960 samples make up the validation set. Our single predictor of sale price will be living area. Restricting our experiment to a single predictor will allow us to more easily visualize the algorithm's behavior. 

A small function was created to randomly sample the training data and split it into the appropriate training and validation sets.
```{python sgd_data, echo=T, eval=T}
X, X_val, y, y_val = data.demo(n=500)
```

Gradient descent is sensitive to feature scaling so let's scale our data to the range [0,1] using sklearn's preprocessing module.
```{python sgd_scale, echo=T, eval=F}
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X = scaler.fit_transform(X)
X_val = scaler.fit_transform(X_val)
y = scaler.fit_transform(y)
y_val = scaler.fit_transform(y_val)
```

With that, let's tune our hyperparameters, starting with the constant learning rate schedule.

```{r sgd_constant, child = '5.2.1 sgd constant.Rmd'}
```