<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return ''+n}
}
}});</script>
#### Stochastic Gradient Descent Constant Learning Rates
Our aim here is to understand the relationship between constant learning rates and objective function performance. Our hyperparameter space below includes a range of learning rates from 0.01 to 2.

```{python sgd_constant_learning_rate_i, echo=T, eval=T}
theta = np.array([-1,-1]) 
learning_rate_sched = ['c']       
learning_rate = np.arange(0.01,2, 0.01)
precision = [0.01]
stop_metric = ['v']
maxiter = 10000
i_s=[5]
```

We stop the algorithm at a maximum of 5000 iterations or when validation set error has not improved in $i_s=5$ consecutive epochs. If we observe underfitting or overfitting on the training data, we may need to make adjustments. 

With that, we are ready to perform our gridsearch.

##### Stochastic Gradient Descent Constant Learning Rate Gridsearch

```{python sgd_constant_learning_rate_filenames, echo=F, eval=T}
sgd_constant_summary_filename = "Stochastic Gradient Descent - Constant Learning Rate Summary.csv"               
sgd_constant_detail_filename = "Stochastic Gradient Descent - Constant Learning Rate Detail.csv"
sgd_constant_eval_filename = "Stochastic Gradient Descent - Constant Learning Rate Eval.csv"
sgd_constant_report_filename = "Stochastic Gradient Descent - Constant Learning Rate Report.csv"
```

```{python sgd_constant_learning_rate_gs_echo, echo=T, eval=F}
lab = SGDLab()
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, learning_rate=learning_rate, 
               learning_rate_sched=learning_rate_sched, time_decay=None, 
               step_decay=None, step_epochs=None, exp_decay=None,
               stop_metric=stop_metric, precision=precision, 
               maxiter=maxiter, i_s=i_s)
```


```{python sgd_constant_learning_rate_gs, echo=F, eval=T, cache=F}
lab = SGDLab()  
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, learning_rate=learning_rate, 
               learning_rate_sched=learning_rate_sched, time_decay=None, 
               step_decay=None, step_epochs=None, exp_decay=None,
               stop_metric=stop_metric, precision=precision, 
               maxiter=maxiter, i_s=i_s)
               
sgd_constant_summary = lab.summary(directory=directory, filename=sgd_constant_summary_filename)
sgd_constant_detail = lab.detail(directory=directory, filename=sgd_constant_detail_filename)
sgd_constant_eval = lab.eval(directory=directory, filename=sgd_constant_eval_filename)
sgd_constant_report = lab.report(directory=directory, filename=sgd_constant_report_filename)
```

```{r sgd_constant_get_gs_r, echo=F, eval=T}
sgd_constant_summary = read.csv(file.path(py$directory, py$sgd_constant_summary_filename))
sgd_constant_detail = read.csv(file.path(py$directory, py$sgd_constant_detail_filename))
sgd_constant_eval = read.csv(file.path(py$directory, py$sgd_constant_eval_filename))
sgd_constant_report = read.csv(file.path(py$directory, py$sgd_constant_report_filename))
```

##### Stochastic Gradient Descent Constant Learning Rate Analysis
`r kfigr::figr(label = "sgd_constant_alpha", prefix = TRUE, link = TRUE, type="Figure")` reveals the effect of learning rate on objective function performance. 

```{python sgd_constant_alpha, echo=F, eval=T}
viz = GradientVisual()
filename = 'Stochastic Gradient Descent Costs by Learning Rate.png'
viz.figure(alg=alg, data=sgd_constant_summary, x='learning_rate', y='final_costs',
           func=viz.lineplot, directory=directory, 
           filename=filename, width=1)
        
```

![](../report/figures/SGD/Lab/Stochastic Gradient Descent Costs by Learning Rate.png)

`r kfigr::figr(label = "sgd_constant_alpha", prefix = TRUE, link = TRUE, type="Figure")`: Stochastic Gradient Descent Training Costs by Learning Rate 

A couple of observations stand out. First, there appears to be a positive correlation between objective function cost and learning rate. We also see an increase in varance of the objective function performance with increasing learning rates. This suggests that SGD favors the relative stability of lower learning rates.  

Let's examine the learning curves as the algorithm approached approximate convergence. 

```{python sgd_constant_best_curve, echo=F, eval=T}
# Get Best Experiment
sgd_constant_report = sgd_constant_report.sort_values(by=['final_costs', 'duration'])
experiment = sgd_constant_report['experiment'].iloc[0]

# Get Detail 
sgd_constant_best_detail = sgd_constant_detail.loc[sgd_constant_detail['experiment']==experiment]
sgd_constant_best_detail = sgd_constant_best_detail.sort_values(by=['epochs', 'iterations'])

# Get Evaluation 
sgd_constant_best_eval = sgd_constant_eval.loc[sgd_constant_eval['experiment']==experiment]
sgd_constant_best_eval = sgd_constant_best_eval.sort_values(by=['epochs', 'iterations'])

sgd_constant_best_detail_filename = 'Stochastic Gradient Descent Learning Curve Detail.png'
sgd_constant_best_eval_filename = 'Stochastic Gradient Descent Learning Curve Smooth.png'

viz.figure(alg=alg, data=sgd_constant_best_detail, x='iterations', y='cost',
           func=viz.lineplot, directory=directory, 
           filename=sgd_constant_best_detail_filename, width=0.5)
viz.figure(alg=alg, data=sgd_constant_best_eval, x='iterations', y='cost',
           func=viz.lineplot, directory=directory, 
           filename=sgd_constant_best_eval_filename, width=0.5)           
        
```

![](../report/figures/SGD/Lab/Stochastic Gradient Descent Learning Curve Detail.png){width=50%}![](../report/figures/SGD/Lab/Stochastic Gradient Descent Learning Curve Smooth.png){width=50%}

`r kfigr::figr(label = "sgd_constant_best_curve", prefix = TRUE, link = TRUE, type="Figure")`: Stochastic Gradient Descent Solution Learning Curves 

The learning curve on the left of `r kfigr::figr(label = "sgd_constant_best_curve", prefix = TRUE, link = TRUE, type="Figure")` is measured at each iteration. On the right, we have the learning curve measured at each epoch. This gives us a better indication of the convergence path, than does the iteration-based learning curve.

Here, the effect of the stochastic updates is extant. Consider the progress towards convergence achieved in the first epoch. SGD makes substantially greater progress per epoch than does batch gradient descent. That's not all.  SGD achieved convergence at a cost of `r round(best_constant['final_costs'][1,],4)` in `r best_constant['epochs'][1,]` epochs. Our experiments have shown batch gradient descent to take upwards of 27 epochs to converge.

Which learning rate resulted in best objective function performance? The best three performing learning rates are listed in `r kfigr::figr(label = "sgd_constant_best", prefix = TRUE, link = TRUE, type="Table")`.

`r kfigr::figr(label = "sgd_constant_best", prefix = TRUE, link = TRUE, type="Table")`: Stochastic Gradient Descent Constant Learning Rate Best Performing Hyperparameters. 
```{r sgd_constant_best, echo=F, eval=T}
best_constant <- sgd_constant_report %>% select(experiment, learning_rate, precision, i_s, maxiter,  epochs, iterations, duration, final_costs) %>% arrange(final_costs, duration)
knitr::kable(head(best_constant,3), digits = 5, format.args = list(big.mark = ",")) %>%
 kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center")
```

We obtained a final objective function cost $J\approx$ `r round(best_constant['final_costs'][1,],4)` (numerical precision issues), with a learning rate $\alpha=$ `r round(best_constant['learning_rate'][1,],2)`, in `r best_constant['epochs'][1,]` epochs, totalling about `r round(best_constant['duration'][1,],4)` milliseconds. It's worth pointing out; however, that the top three learning rates performed almost identically. In fact, `r kfigr::figr(label = "sgd_constant_alpha", prefix = TRUE, link = TRUE, type="Figure")` indicates that there are learning rates across the spectrum that produced practically equivalent results. 

##### Stochastic Gradient Descent Constant Learning Rate Solution Path 
The solution path below amplifies `r kfigr::figr(label = "sgd_constant_best_curve", prefix = TRUE, link = TRUE, type="Figure")`.
```{python sgd_constant_learning_rate_solution, echo=F, eval=T, cache=F}

sgd_constant_best_summary = sgd_constant_summary.loc[sgd_constant_summary['experiment']==experiment]

sgd_constant_search_filename = 'Stochastic Gradient Descent Constant Learning Rate Search Path.gif'
sgd_constant_fit_filename = 'Stochastic Gradient Descent Constant Learning Rate Regression Fit.gif'

sgd = SGD()
X_scaled, y_scaled = sgd.prep_data(X,y)
viz = GradientVisual()
viz.show_search(alg, X_scaled, y_scaled, sgd_constant_best_eval, sgd_constant_best_summary, 
               directory=directory, filename=sgd_constant_search_filename, fps=1)
viz.show_fit(alg, X_scaled, y_scaled, sgd_constant_best_eval, sgd_constant_best_summary, 
               directory=directory, filename=sgd_constant_fit_filename,fps=1)               
```

![](../report/figures/SGD/Lab/Stochastic Gradient Descent Constant Learning Rate Search Path.gif)

`r kfigr::figr(label = "sgd_constant_learning_rate_convergence", prefix = TRUE, link = TRUE, type="Figure")`: Stochastic Gradient Descent Constant Learning Rate Solution Path

We observe rapid and significant progress towards convergence in the first epoch, then a sharp turn in the direction of positive slope ($\theta_1$). The majority of the optimization time was spent in the $\theta_1$ dimension.

How does the solution fit the data?  With zero cost, we should assume pretty well.  Let's see.

![](../report/figures/SGD/Lab/Stochastic Gradient Descent Constant Learning Rate Regression Fit.gif)
`r kfigr::figr(label = "sgd_constant_learning_rate_fit", prefix = TRUE, link = TRUE, type="Figure")`: Stochastic Gradient Descent Constant Learning Rate Solution Fit

As already stated, the first epoch put us in a good neighborhood with respect to the optimal solution. A bit of fine tuning rendered a reasonably good fit to the data. 

How does the solution generalize?  Let's see.

##### Stochasetic Gradient Descent Constant Learning Rate Evaluation


```{python sgd_constant_eval, echo=F, eval=T}
viz = GradientVisual()
filename = 'Stochastic Gradient Descent Constant Learning Rate Evaluation.png'
viz.figure(alg=alg, data=sgd_constant_summary.head(5), x='learning_rate', y='final_costs',
           func=viz.barplot, directory=directory, 
           filename=filename, width=1)
        
```