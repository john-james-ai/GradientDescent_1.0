<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return ''+n}
}
}});</script>
#### Constant Learning Rates
Our aim here is to understand the relationship between learning rate and objective function performance, examine the effect of various early stop criteria on the optimization problem, and determine which hyperparameters minimize empirical (training set costs) and expected (validation set error) risk.

Once we've specified our hyperparameters, we will execute an exhaustive gridsearch to reveal SGD algorithm behavior within our hyperparameter space. 

##### Hyperparameter Space
Our hyperparameter space specified below spans a range of learning rates and early stop parameters. 
```{python sgd_constant_learning_rate_i, echo=T, eval=T}
theta = np.array([-1,-1]) 
learning_rate_sched = ['c']       
learning_rate = np.arange(0.01,2, 0.01)
precision = [0.001, 0.01, 0.1]
stop_metric = ['j', 'v', 'g']
maxiter = 5000
i_s=[1,2,5,10]
```

With that, we are ready to perform our gridsearch.

##### Constant Learning Rate Gridsearch

```{python sgd_constant_learning_rate_filenames, echo=F, eval=T}
constant_summary_filename = "Stochastic Gradient Descent - Constant Learning Rate Summary.csv"               
constant_detail_filename = "Stochastic Gradient Descent - Constant Learning Rate Detail.csv"
```

```{python sgd_constant_learning_rate_gs_echo, echo=T, eval=F}
lab = SGDLab()
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, learning_rate=learning_rate, 
               learning_rate_sched=learning_rate_sched, time_decay=None, 
               step_decay=None, step_epochs=None, exp_decay=None,
               stop_metric=stop_metric, precision=precision, 
               maxiter=maxiter, i_s=i_s)
```


```{python sgd_constant_learning_rate_gs, echo=F, eval=T, cache=T}
lab = SGDLab()
lab.gridsearch(X=X, y=y, X_val=X_val, y_val=y_val, theta=theta, learning_rate=learning_rate, 
               learning_rate_sched=learning_rate_sched, time_decay=None, 
               step_decay=None, step_epochs=None, exp_decay=None,
               stop_metric=stop_metric, precision=precision, 
               maxiter=maxiter, i_s=i_s)
               
constant_summary = lab.summary()
constant_summary.to_csv(os.path.join(directory, constant_summary_filename))
constant_detail = lab.detail()
constant_summary.to_csv(os.path.join(directory, constant_detail_filename))
```

```{python sgd_constant_get_gs, echo=F, eval=T}
lab = SGDLab()
data = pd.read_csv(os.path.join(directory, constant_summary_filename))
```

Next, we'll analyze the effect of learning rate on our optimization problem.  

##### Learning Rate Analysis
The aim here is to get a general sense of the effect of learning rate on objective function performance. 
```{python sgd_constant_alpha, echo=F, eval=T}
lab = SGDLab()
data = pd.read_csv(os.path.join(directory, constant_summary_filename))
filename = 'Stochastic Gradient Descent Costs by Learning Rate.png'
lab.figure(data=data, x='learning_rate', y='final_costs',
           func=lab.lineplot, directory=directory, 
           filename=filename, width=1)
        
```

![](../report/figures/SGD/Lab/Stochastic Gradient Descent Costs by Learning Rate.png)

`r kfigr::figr(label = "sgd_constant_alpha", prefix = TRUE, link = TRUE, type="Figure")`: Stochastic Gradient Descent Training Costs by Learning Rate 

A couple of observations stand out. First, there appears to be a positive correlation between objective function cost and learning rate. We also see an increase in varance of the objective function performance with increasing learning rates. This suggests that SGD favors the relative stability of lower learning rates.  

With that, let's drill down a bit and investigate the effect of our stopping criteria on the optimization problem vis-a-vis learning rate. We'll start with our stop metric hyperparameter.

##### Stop Criteria Analysis
Our aim here is to evaluate the effects of various early stop hyperparameters on objective function performance. Recall, that we evaluate convergence based upon improvement (or change) in the following stop metrics:  

* 'j' = training set costs,    
* 'v' = validation set error, and    
* 'g' = magnitude of the gradient.

Improvement is measured with a tolerance, or precision parameter $\epsilon=$ [0.001,0.01, 0.1]. The algorithm stops if there is no improvement or change in the stop metric for $i_s=$ [1,2,5,10] consecutive iterations.

For these parameters, we will evaluate the effect on objective function performance in terms of the correlation between each hyperparameter and objective function costs.

```{python sgd_constant_stop_criteria_correlation, echo=F, eval=T}
filename = 'Stochastic Gradient Descent Stop Criteria Correlations.png'
x = ['stop_metric', 'precision', 'i_s']
y = ['final_costs']
associations = lab.associations(data=data,x=x,y=y)
```
`r kfigr::figr(label = "sgd_constant_param_corr", prefix = TRUE, link = TRUE, type="Table")`: Stochastic Gradient Descent Parameter Correlation with Objective Function Performance
```{r sgd_constant_param_corr, echo=F, eval=T}
associations <- py$associations
rownames(associations) <- NULL
knitr::kable(associations, digits = 4, format.args = list(big.mark = ",")) %>%
 kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center")
```
